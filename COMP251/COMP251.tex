% Created 2017-11-16 Thu 12:32
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{algpseudocode, wasysym, mathtools}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.3.1 (Org mode 9.1.2)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

\section{Lecture 1 \textit{<2017-09-05 Tue>}}
\label{sec:orgbc85584}
\subsection{Algorithm}
\label{sec:orge5dfda7}
\begin{itemize}
\item Al-Khwarizmi (9th Century)
\item Algorismus (Latin)
\item Arithmos (Greek)
\begin{itemize}
\item Greek + Latin > Algorithm
\end{itemize}
\end{itemize}
A set of step by step instructions
\begin{enumerate}
\item Every step simple and precise
\item Produces an answer in finite time (not run forever)
\end{enumerate}
This course will be very rigorous, lots of proofs, but it will take 2-3 months to formally define algorithms, so we'll just have to be satisfied with thsi definition.
Formalized in 1930's by Turing and Church.
\begin{itemize}
\item Covered in COMP 330
\end{itemize}
Even though the concept of an algorithm is very simple and intuitive, it's not very obvious to prove things.
\begin{itemize}
\item Algorithms are an old concept, have been studied forever. Some examples are really old
\end{itemize}
\subsubsection{Examples of algorithms}
\label{sec:org0d58df5}
\begin{itemize}
\item Recipes
\item 1600 BC Babylonians (Factorization and square roots)
\item Euclid's Algorithm (200 BC)
\begin{itemize}
\item Finding greatest common divisor of 2 numbers
\end{itemize}
\end{itemize}
Field of theoretical computer science is much older than first computers.
\begin{itemize}
\item Really mature field.
\item Computers are just a device that helps us use these things.
\item Theoretical computer science is part of math and science and has been studied for milennials
\end{itemize}
\subsection{Teacher's website}
\label{sec:orgee96957}
\url{http://www.cs.mcgill.ca/\~hatami/}
\begin{itemize}
\item He will be following the textbook.
\end{itemize}
\subsection{Stable matching}
\label{sec:org711f553}
n men: \(m_1, m_2, \ldots, m_n\)
n women: \(w_1, w_2, \ldots, w_n\)

Every man and woman has a ranking of people of other gender.
\subsubsection{Ex: \(n=4\)}
\label{sec:orgbe16c5e}
\(m_1: w_3 > w_1 > w_2 > w_4\)

\(m_2: w_1 > w_4 > w_2 > w_3\)

\(m_3: w_1 > w_2 > w_4 > w_3\)

\(m_4: w_2 > w_3 > w_4 > w_1\)

\noindent\rule{\textwidth}{0.5pt}

\(w_1: m_4 > m_2 > m_1 > m_3\)

\(w_2: m_1 > m_2 > m_3 > m_4\)

\(w_3: m_2 > m_1 > m_3 > m_4\)

\(w_4: m_4 > m_1 > m_2 > m_3\)

\begin{enumerate}
\item A pairing
\label{sec:orge18ccbe}
\begin{itemize}
\item \(m_1+w_2\)
\item \(m_2+w_4\)
\item \(m_3+w_1\)
\item \(m_4+w_3\)
\end{itemize}
What is unstable about this? The last pair, \(m_4\) and \(w_3\).
\begin{itemize}
\item \(m_1\) and \(w_3\) prefer each other
\end{itemize}
\item Unstability:
\label{sec:org532a3d8}
If there is a pair \((m,w)\) such that
\begin{enumerate}
\item m prefers w to his current partner
\item w prefers m to her current partner
\item Selfish agents, everyone wants to be with the best possible partner they can find
\end{enumerate}
\item Problem:
\label{sec:orgc8f8328}
Can we find a stable matching?
\end{enumerate}
\subsection{Stable Matching Algorithm}
\label{sec:org213cf65}
while \(\exists\) a free man \uline{\(m\)}
\begin{itemize}
\item \(m\) proposes to the highest-ranked woman \uline{\(w\)} that he has not prosed yet
\item If \uline{\(w\)} is free \uline{or} prefers \(m\) to her current partner, she gets engaged to \uline{\(m\)} and her current partner becomes free
\end{itemize}
else
\begin{itemize}
\item She rejects \uline{\(m\)} and \uline{\(m\)} remains free
\end{itemize}
End while

\subsubsection{For our example:}
\label{sec:org46604a2}
\begin{itemize}
\item \(m_1\) proposes to \(w_3\), accepts > \(m_1+w_3\)
\item \(m_2\) proposes to \(w_1\), accepts > \(m_2+w_1\)
\item \(m_3\) proposes to \(w_1\), rejects
\begin{itemize}
\item \(m_3\) proposes to \(w_2\), accepts > \(m_3+w_2\)
\end{itemize}
\item \(m_4\) proposes to \(w_2\), rejects
\begin{itemize}
\item \(m_4\) proposes to \(w_3\), rejects
\item \(m_4\) proposes to \(w_4\), accepts > \(m_4+w_4\)
\end{itemize}
\end{itemize}
Simple example, no one broke up.
Let's change the example a bit.
\subsubsection{Modified example}
\label{sec:org598b776}
\(m_1: w_3 > w_1 > w_2 > w_4\)

\(m_2: w_1 > w_4 > w_2 > w_3\)

\(m_3: w_1 > w_2 > w_4 > w_3\)

\(m_4: w_2 > w_3 > w_4 > w_1\)

\noindent\rule{\textwidth}{0.5pt}

\(w_1: m_4 > m_2 > m_1 > m_3\)

\(w_2: m_1 > m_2 > m_3 > m_4\)

\(w_3: m_2 > m_4 > m_3 > m_1\)

\(w_4: m_4 > m_1 > m_2 > m_3\)

\begin{itemize}
\item \(m_1\) proposes to \(w_3\), accepts
\item \(m_2\) proposes to \(w_1\), accepts
\item \(m_3\) proposes to \(w_1\), rejects
\begin{itemize}
\item \(m_3\) proposes to \(w_2\), accepts
\end{itemize}
\item \(m_4\) proposes to \(w_2\), rejects
\begin{itemize}
\item \(m_4\) proposes to \(w_3\), accepts, breaks up with \(m_1\)
\end{itemize}
\item \(m_1\) proposes to \(w_1\), rejects
\begin{itemize}
\item \(m_1\) proposes to \(w_2\), accepts, breaks up with \(m_3\)
\end{itemize}
\item \(m_3\) proposes to \(w_4\), accepts
\end{itemize}

\subsubsection{Why isn't this infinite?}
\label{sec:org0037ba6}
\(P(t)\): Number of pairs \((m,w)\) such that \(m\) has not proposed to \(w\) yet at time \(t\) (number of iterations of while loop).
\(P(0)=n^2\)
\(P(1)=n^2-1\)
Is it possible that a man proposes to a woman more than once? No.
\begin{enumerate}
\item Fact:
\label{sec:org704ceb9}
No man proposes to the same woman more than once.

Some of these proposals may never happen.

The quantity \(P\) will never go negative.
\item Fact:
\label{sec:org9bb057e}
\(P(t)\) decreases by \(1\) at every iteration.
\item Lemma:
\label{sec:org771f9c4}
The algorithm terminates after at most \(n^2\) iterations. There will be no free men at the end.

\item Fact:
\label{sec:orgbd6d99b}
Once a woman gets a proposal, she is never free again.

\(\implies\) If a man \uline{\(m\)} remains free by the end of the alg it means that at the end all women are engaged.
\(\implies\) Since there are \(n\) men and \(n\) women this means that all men are engaged as well.

\(\implies\) At the end every person is engaged.
\begin{itemize}
\item This algorithm gives us a pairing.
\begin{itemize}
\item But we need to show that this is a good pairing, that it's stable.
\end{itemize}
\end{itemize}
\end{enumerate}
\section{Lecture 2 \textit{<2017-09-07 Thu>}}
\label{sec:org92de87f}
\subsection{Announcements}
\label{sec:org691cc65}
\begin{itemize}
\item Lectures will be recorded.
\item Assignment 1 to come out soon, probably early next week.
\end{itemize}
\subsection{Recall:}
\label{sec:org0a75d76}
Stable matching \(n\) men \(n\) women.
\begin{itemize}
\item Not a fundamental problem, but contains many of the elements we'll see later in this course
\end{itemize}
\subsubsection{Ex: \(n=4\)}
\label{sec:org81ac653}
\begin{center}
\begin{tabular}{lllll}
Man & Preference 1 & 2 & 3 & 4\\
\hline
\(m_1\): & \(w_3>\) & \(w_1>\) & \(w_2>\) & \(w_4\)\\
\(m_2\): & \(w_1>\) & \(w_4>\) & \(w_2>\) & \(w_3\)\\
\(m_3\): & \(w_1>\) & \(w_2>\) & \(w_4>\) & \(w_3\)\\
\(m_4\): & \(w_2>\) & \(w_3>\) & \(w_4>\) & \(w_1\)\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{lllll}
Woman & Pref 1 & 2 & 3 & 4\\
\hline
\(w_1\): & \(m_4>\) & \(m_2>\) & \(m_1>\) & \(m_3\)\\
\(w_2\): & \(m_1>\) & \(m_2>\) & \(m_3>\) & \(m_4\)\\
\(w_3\): & \(m_2>\) & \(m_4>\) & \(m_3>\) & \(m_1\)\\
\(w_4\): & \(m_4>\) & \(m_1>\) & \(m_2>\) & \(m_3\)\\
\end{tabular}
\end{center}

Same example as last lecture, see matching/use of algorithm in lecture 1.
Matching becomes: 

\begin{center}
\begin{tabular}{llll}
\(m_1\) & \(m_2\) & \(m_3\) & \(m_4\)\\
\hline
\(w_2\) & \(w_1\) & \(w_4\) & \(w_3\)\\
\end{tabular}
\end{center}

Top matched with bottom. Does \(w_1\) have a tendancy to break up and go with \(m_3\)? No.

\subsubsection{Last lecture we proved:}
\label{sec:orgd466353}
\begin{enumerate}
\item The algorithm always terminates.
\begin{itemize}
\item Easy to see from the list of preferences, because we go down the list of the men's preferences, they always go down their list and never go back
\end{itemize}
\item When the algorithm terminates everybody has a partner.
\begin{itemize}
\item Won't end up with a situation where a man proposes to everyone and gets rejected
\item Women will never be free once they are initially proposed to
\item A man can't be free at the end, because that means all women we're proposed to and all women are married
\begin{itemize}
\item But same amount of women and men
\end{itemize}
\end{itemize}
\end{enumerate}

\noindent\rule{\textwidth}{0.5pt}
\subsection{Stable Matching Algorithm}
\label{sec:org3ec72f5}
\subsubsection{Does this algorithm produce a stable marriage?}
\label{sec:org58427b7}
It remains to show that the output is stable.
\begin{enumerate}
\item Observation 1
\label{sec:org461fe5d}
\begin{itemize}
\item Throughout the algorithm every man's partner gets worse and worse
\end{itemize}
\item Observation 2
\label{sec:org6c48b33}
\begin{itemize}
\item However, for women it is the opposite
\item They accept the first proposal
\item But every partner gets better and better
\end{itemize}
\item Theorem:
\label{sec:org815c2a6}
The final matching is stable.
\begin{enumerate}
\item Proof:
\label{sec:org6f12bee}
Suppose not! Then there exists engaged pairs as in the following:
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i1.png}
\end{center} 
But in this case \(m\) would have proposed to \(w\) before proposing to \(w'\), and as a result we know that \(w\) would not have ended up with someone worse than \(m\).
\end{enumerate}
\end{enumerate}

\subsubsection{Is this algorithm better for men or women?}
\label{sec:org1b93e39}
Let's say \((m,w)\) is valid if there exists \uline{some} stable matching that pairs \(m\) and \(w\).

\begin{enumerate}
\item Fact:
\label{sec:org4fefd9c}
This algorithm matches every man with their most preferred valid \(w\) and every woman with their least preferred valid \(m\).
\begin{itemize}
\item For men, they start ambituously and go for their most preferred partner and go down the list if needed
\item For women, they start at whatever is first given and only improve if needed
\item Formal proof in textbook, won't do it in class as to spend less time on this problem
\end{itemize}
\end{enumerate}

\subsection{Notes on problems}
\label{sec:org6325060}
\begin{itemize}
\item Formulate the problem as a precise mathematical problem.
\begin{itemize}
\item What is the input?
\item What is the goal?
\item Conditions we want to satisfy?
\item Everything must be precise or else we won't be able to satisfy all these things.
\end{itemize}
\item Design an algorithm
\item Analyze the algorithm:
\begin{itemize}
\item It always terminates
\begin{itemize}
\item Show that, no matter the input, it will always stop, no infinite loop
\end{itemize}
\item It outputs the correct output!
\begin{itemize}
\item In stable marriage, we showed that it is always stable
\end{itemize}
\item Running time
\begin{itemize}
\item How long does it take to terminate?
\begin{itemize}
\item For stable marriage, we could brute force and try all possible combinations and see if they're stable or not, but that would be \(n!\)
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

Professor won't do much on first point, about formulating problem as math. Textbook often presents the problem in a bunch of sentences for some real life thing and we need to extract the mathematical problem from there, which the professor isn't a big fan of.

\subsection{Some example problems}
\label{sec:org5f8bbf8}
\subsubsection{Interval scheduling}
\label{sec:orged233f5}
\begin{itemize}
\item Let's say you have a room and want to rent it out
\item Bunch of offers that say the person wants to use the room from a start time to an end time
\item Want to accomodate as many people as possible, but we can't have overlap
\begin{itemize}
\item Maximize number of offers without overlap
\end{itemize}
\end{itemize}

\begin{enumerate}
\item Input:
\label{sec:org7fff0e8}
\begin{itemize}
\item \(n\) requests
\item Starting time \(s_1, s_2, \ldots , s_n\)
\item Finishing time \(f_1, f_2, \ldots, f_n\)
\item \uline{Such that} \(s_i<f_i\)
\end{itemize}

\item Problem
\label{sec:org84be999}
We want to pick the max number of these tasks s.t. no two overlap. (Maximum bookings, not maximum time, not charging per hour)
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i2.png}
\end{center}
\begin{enumerate}
\item Algorithm?
\label{sec:org36f024c}
\begin{itemize}
\item What algorithm is good for this?
\item Pick next available room that finishes the earliest and keep going
\item \textbf{Greedy algorithm}
\end{itemize}
\end{enumerate}
\end{enumerate}
\subsubsection{Weighted Interval Scheduling}
\label{sec:org0d2b67c}
\begin{itemize}
\item Now every offer comes with some value.
\item \(v_1,\ldots,v_n\)
\begin{itemize}
\item where \(v_i\) is the value we get from accomodating the \(i^{th}\) offer.
\end{itemize}
\item \(s_1,\ldots,s_n\)
\item \(f_1,\ldots,f_n\)
\end{itemize}
Want higher value, rather than most matchings
Why is this harder to solve than the previous problem? Because the previous one is a special case of the first.
\begin{itemize}
\item Reduction = reducing this problem to the previous to get an answer
\item Setting \(v_1=\ldots=v_n=1\) solves the previous problem.
\end{itemize}
We will solve this using \textbf{Dynamic Programming}
\begin{itemize}
\item Create huge table, keep filling it up as you process input
\begin{itemize}
\item Solve solution for smaller version of problem and keep expanding based on that
\end{itemize}
\item Let's say \(A[t]=\) max value if we stop at time \(t\).
\end{itemize}
\begin{enumerate}
\item Independent set problem
\label{sec:orgd181a9d}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i3.png}
\end{center}
Independent set: A set of notes, no two are adjacent.
Find the largest independent set.
\begin{itemize}
\item Obvious way of doing it without concerning ourselves with time?
\begin{itemize}
\item Brute force
\end{itemize}
\item Without that? You can do some heuristics, but,
\begin{itemize}
\item It is widely believed that every algorithm for this problem is of brute force nature: It is more or less checking all the possible subsets?
\end{itemize}
\end{itemize}
\begin{enumerate}
\item P vs NP?
\label{sec:orge72f63b}
\begin{itemize}
\item Most important problem in computer science
\item Common belief: \(P\neq NP\)
\item This is an example of a problem which is believed to be NP
\end{itemize}

So that is essentially a small instruction about the types of problems we'll be seeing in this course. Next lecture we'll be formally going through running time.
\end{enumerate}
\end{enumerate}
\section{Lecture 3 \textit{<2017-09-12 Tue>}}
\label{sec:org60716ac}
\subsection{Running Time Analysis}
\label{sec:org76a391b}
\begin{itemize}
\item We will be talking about running time of an algorithm.
\end{itemize}
\subsubsection{Questions}
\label{sec:org3f9ca28}
Thinking back without knowledge of running time, what questions can we pose?

\begin{itemize}
\item How should we measure the running time of an algorithm?
\item How can we compare the efficiency of two algorithms?
\item What should we call an \uline{efficient} algorithm?
\begin{itemize}
\item Brute force isn't efficient for finding a matching.
\item Was our algorithm for stable matchings efficient?
\item We want to understand the concept of efficiency for an algorithm.
\end{itemize}
\end{itemize}
\begin{enumerate}
\item One option:
\label{sec:orgb819826}
\begin{itemize}
\item Call an algorithm efficient if it performs "fast" on \uline{"real world"} inputs.
\begin{itemize}
\item What is a real world input?
\begin{itemize}
\item Without a good/rigorous definition, then this isn't a good option.
\item Not precise, so this option doesn't work.
\end{itemize}
\end{itemize}
\end{itemize}
\item Option II:
\label{sec:orgc6e2a4c}
\begin{itemize}
\item Take the set of all inputs of a certain size and take the average \uline{running time} of our algorithm on them.
\begin{itemize}
\item Maybe the inputs we care about are quite sparse in the set of all inputs.
\item Random inputs might be quite trivial
\begin{itemize}
\item May lead us to think we defined a good algorithm
\item But in reality what we care about is harder
\end{itemize}
\end{itemize}
\end{itemize}
\item Example: Algorithm for prime numbers
\label{sec:org6051ee0}
\begin{itemize}
\item Input: integer \(n\)
\item Output: Is \(n\) a prime number?
\begin{itemize}
\item Alg 1:
\begin{algorithmic}
	\For{$i=2$ to $n-1$}
		   \If{$n \pmod{i}=0$} return False
		   \EndIf
	\EndFor
	\State return true
\end{algorithmic}
\item Look at all the numbers between \(1,\ldots, N\)
\item How many are divisible by \({2,3,4,5,6,7}\)? \(1-\frac{1}{2}\times \frac{1}{3}\times \frac{1}{5} \times \frac{1}{7}>99\%\)
\item On average performs well
\item Worst case (prime numbers) does not perform well.
\item While this notion of average time complexity is useful, because the majority of inputs dominate the worst case ones, it is not a very good definition.
\end{itemize}
\item Better to just care about the worst case
\end{itemize}
\item Worst case time analysis
\label{sec:orgafe38a6}
We measure the \uline{running time} against the worst input of a given \uline{size}
\begin{itemize}
\item Want to be inddependent of implementation:
\begin{itemize}
\item We will count the number of "simple steps" (e.g. \uline{If "\(a>b\)"}, \(a:=b \times c\))
\end{itemize}
\end{itemize}
\end{enumerate}
\subsubsection{Efficiency}
\label{sec:orgdc0d596}
\begin{enumerate}
\item \underline{Def:}
\label{sec:org044a1ea}
We call an algorithm \textbf{efficient} if its running time is bounded by a polynomial \(P(n)\) for every input of \uline{size} (in number of bits) \(n\)
\begin{itemize}
\item \(n\) efficient
\item \(n^2\) good
\item \(n \log n\) good
\item \(2^n\) bad
\item \(n \log n < n^2\)
\end{itemize}
Remember that you need \(\log{n}\) bits to store a number \(n\).
\begin{itemize}
\item Objection: \uline{\(n^{100}\)} is considered efficient while it is not practical!
\item Answer: Usually the exponents are better. (Rarely see \(n^{100}\) if ever)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
\begin{itemize}
\item Scales well
\begin{itemize}
\item Many interesting algorithms have polynomial time algorithms
\end{itemize}
\end{itemize}
\item \underline{Alternative Def:}
\label{sec:orgdb8bcea}
Efficient is running time \(<n^3\) seems a better def as it overrules cases like \(n^{100}\)
\begin{itemize}
\item Let's say you're combining 2 algorithms, say you're running a \(n^2\) algorithm in a \(n^2\) for-loop
\begin{itemize}
\item Suddenly you're stuck with an \(n^4\) algorithm
\item This doesn't allow us to easily stick algorithms in for-loops and the like
\end{itemize}
\item This is not very robust.
\begin{itemize}
\item The choice of data structure, pseudo-code, \ldots can change the running time a bit and so this definition is not \uline{"robust"}. Result depends on implementation.
\end{itemize}
\end{itemize}
\item Example:
\label{sec:org1e930fb}
Input: An array \(A[0\ldots n-1]\)

Goal: Are all elements in \(A[]\) distinct?
\begin{algorithmic}
\For{$i=0$ to $n-2$}
	   \For{$j=i+1$ to $n-1$}
	   	\If{$A[i]==A[j]$}
			\State return "False"
		\EndIf
	   \EndFor
\EndFor
\State Return "True"
\end{algorithmic}
\begin{center}
\begin{tabular}{ll}
Step & Iterations\\
\hline
\(c_1\): setting \(i\) & \(n-1\)\\
\(c_2\): setting \(j\) & \(\sum_{i=0}^{n-2}\sum_{j=i+1}^{n-1}1=\frac{n(n-1)}{2}\)\\
\(c_3\): comparing \(A[i]==A[j]\) & \(\frac{n(n-1)}{2}\)\\
\(c_4\): return False & \(1\)\\
\(c_5\): return True & \(1\)\\
\end{tabular}
\end{center}

Running time: 
\begin{align*}
& n-1+\frac{n(n-1)}{2}+\frac{n(n-1)}{2}+1+1 = n^2+1
\end{align*}
\uline{Efficient}

This much accuracy is \uline{meaningless}: Each one of these commands consist of some more primitive commands and that can depend on your compiler, \ldots
\begin{itemize}
\item What matters is that this is quadratic.
\end{itemize}
\end{enumerate}

\subsubsection{Big-O notation}
\label{sec:org9b0b343}
Informally \(O(g(n))\) is the set of all functions with smaller or same order of growth.
\begin{itemize}
\item You should think of it as a set, not a value.
\item \(n \in O(n^2)\)
\item \(100n+5 \in O(n^2)\)
\item \(\frac{1}{2}n(n-1)\in O(n^2)\)
\item \(n^3 \notin O(n^2)\)
\end{itemize}
\begin{enumerate}
\item Def:
\label{sec:orgff75482}
\(f(n)\in O(g(n))\) if \(\exists n_0, c > 0\) such that \(f(n)<cg(n)\)  \(\forall n>n_0\)
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i4.png}
\end{center}
\item Ex:
\label{sec:org261349d}
\(100n+5 \in O(n^2)\)
\begin{enumerate}
\item Proof
\label{sec:orgbc29f0d}
\(100n+5 \stackrel{n\geq 5, n_0=5}{\leq} 100n+n \leq \underbrace{101}_{c=101}n\)
\end{enumerate}
\end{enumerate}

\subsubsection{\(\Omega\)-notation:}
\label{sec:org4329786}
Informally \(f(n)\in \Omega(g(n))\) if \(f(n)\) grows faster or the same as \(g(n)\)
\begin{enumerate}
\item Def:
\label{sec:orgcd02a08}
\(f(n) \in \Omega(g(n))\) if \(\exists n_0, c > 0\) such that \(f(n) \geq cg(n)\)  \(\forall n\geq n_0\)

(Equivalently \(g(n)\in O(f(n))\))

\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i5.png}
\end{center}

\item Ex:
\label{sec:org3884c97}
\(\frac{n^2}{2}-5n\in \Omega(n^2)\)
\(\frac{n^2}{2}-5n \geq \frac{1}{4} n^2 \implies c=\frac{1}{4}\)
\(\forall n \geq 20 = n_0\)
\end{enumerate}
\section{Lecture 4 \textit{<2017-09-14 Thu>}}
\label{sec:orgf193cae}
\subsection{Recall:}
\label{sec:org1506efa}
\begin{itemize}
\item Big-Oh
\item Omega notation
\end{itemize}
\subsection{Examples}
\label{sec:org96521e3}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i6.jpg}
\end{center}
\subsection{\(\Theta\)-notation:}
\label{sec:orgc9363e8}
\(f(n)\iff f(n) = O(g(n))\) and \(f(n)=\Omega(g(n))\)
\begin{itemize}
\item Grows at the same rate as \(g(n)\)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Alternatively:

\(\exists n_0, c_1, c_2 \forall n>n_0\), s.t. \(c_1g(n)\leq f(n) \leq c_2 g(n)\)
\subsubsection{Examples}
\label{sec:org6970c95}
\begin{itemize}
\item \(2n^2+1 = \Theta(n^2)\)
\begin{enumerate}
\item \(n^2-5n+10 \leq n^2 \forall n \geq 2\)
\item \(n^2-5n+10 \geq \frac{n^2}{2} \forall n \geq 20\)
\end{enumerate}
\end{itemize}
\subsection{Theorem}
\label{sec:orgd853d83}
Let \(f(n)=a_d n^d + a_{d-1}n^{d-1}+\ldots+a_1 n + a_0, a_d>0\).

Then \(f(n)=\Theta(n^d)\)
\subsubsection{Proof}
\label{sec:org98130ae}
\begin{itemize}
\item \((f(n)=O(n^d)\)
\begin{itemize}
\item \(f(n)=a_d n^d + \ldots + a_1n + a_0 \leq \underbrace{(a_d+|a_{d-1}+\ldots+|a_0|)}_c n^d\), \(\forall n\geq 1\)
\item E.g. \(2n^2-5n+10 \leq (2+5+10)n^2\)
\end{itemize}
\item \(f(n)=\Omega(n^d)\)
\begin{itemize}
\item \(a_d n^d + a_{d-1}n^{d-1}+\ldots + a_1 n + a_0 \geq C n^d\)
\item \(c=\frac{a_d}{2}\), since \(a_d\) is controlling the growth rate of the left hand side.
\item \(\frac{a_d}{2}n^d \geq - (a_{d-1}n^{d-1}+a_{d-2}n^{d-2}+\ldots+a_0)\)
\item \(\frac{a_d}{2}n^d \geq (|a_{d-1}|+\ldots+|a_0|)n^{d-1}\), \(\forall n\geq \frac{2(|a_{d-1}+\ldots+|a_0|)}{a_d}\) (by rearranging and isolating \(n\))
\item On the other hand:
\begin{itemize}
\item \((|a_{d-1}|+\ldots+|a_0|)n^{d-1} \geq - (a_{d-1}n^{d-1}+\ldots+a_0)\)
\end{itemize}
\item Note that \(|a_r|n^{d-1} \geq -a_r n^r, r\leq d-1\)
\end{itemize}
\end{itemize}
\subsection{Little o and Little omega}
\label{sec:org5841814}
\begin{itemize}
\item Show strict upper and lower bounds, rather than equalities
\end{itemize}
\(f(n)=o(g(n))\)
\begin{itemize}
\item \(\lim_{n\to \infty}\frac{f(n)}{g(n)}=0\)
\item Little oh implies big-Oh, but not the other way around
\end{itemize}
\(f(n)=\omega (g(n)\)
\begin{itemize}
\item \(\lim_{n\to \infty} \frac{g(n)}{f(n)} = 0\)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
\(n^{1/100}\) vs \(\log_2 (n)^5\) 

Claim: \(\log_2(n)^5 = o(n^{1/100})\)

Proof: \(\lim_{n\to \infty}\frac{\log_2(n)^5}{n^{1/100}} = \lim_{n\to\infty}\frac{5\log(n)^4 \frac{\ln(2)}{n}}{\frac{1}{100}n^{\frac{-99}{100}}} = \ldots = 0\) (have to do L'Hopital's 4 more times)

The lesson is that anything in log grows much slower than any polynomial.
\subsection{Theorem}
\label{sec:org370af2b}
\begin{itemize}
\item \(\forall r>1, d>0\)
\item \(n^d = o(r^n)\) (i.e. polynomials grow much slower than exponential functions)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
\(\underbrace{n^{10000}}_{\text{Better}}\) vs \(1.0001^{n}\)
\subsection{Stable Marriage}
\label{sec:orgdf376b9}
Data structures we may use:
\begin{itemize}
\item Array \(A[0\ldots n-1]\)
\begin{itemize}
\item Operation times:
\begin{itemize}
\item Access \(A[i]: O(1)\)
\item Insert a new entry somewhere in the middle: \(O(n)\), need to shift.
\item Delete: \(O(n)\)
\item Finding an element: \(O(n)\) not sorted
\begin{itemize}
\item \(O(\log(n))\) sorted
\end{itemize}
\end{itemize}
\end{itemize}
\item Linked List
\begin{itemize}
\item Operation times:
\begin{itemize}
\item Access \(i-th\) entry: \(O(n)\)
\item Insert-delete: \(O(1)\)
\item Finding: \(O(n)\)
\end{itemize}
\end{itemize}
\end{itemize}

\begin{algorithmic}
\While {$\exists$ a free man $m$}
       \State Let $w$ be the highest-ranked woman $m$ has not proposed to yet.
       \If {$w$ is free}
       	   \State $(m,w)$ engaged
	\ElsIf{$w$ is currently engaged to $m'$}
		  \If {$w$ prefers $m$ to $m'$}
		      \State $(m',w)$ engaged
		      \State $m$ becomes free
		      \EndIf
	\EndIf	 
\EndWhile
\end{algorithmic}
\begin{itemize}
\item Input: Two (men and women) \(n\times n\) arrays (rankings)
\item Reading input \(\Theta(n^2)\) so at best we can hope \(\Theta(n^2)\) for the alg.
\item The main while loop can repeat \(O(n^2)\) times \(\implies\) To have total \(\Theta(n^2)\) time every iteration must be done in \(O(1)\).
\item How to implement?
\begin{itemize}
\item When do we know if a man is free?
\begin{itemize}
\item Can have an array of booleans of free men, but then you need for loop to check if there's a free man, which will be \(O(n)\)
\item Solution 1: Can have a linked list of free men.
\begin{itemize}
\item Delete someone from the list when they get engaged.
\item Deleting and adding is \(O(1)\) (add to front)
\end{itemize}
\item Solution 2: Using an array
\begin{itemize}
\item Have a pointer to first free man and another to last free man
\item If first man gets engaged, move pointer to the right
\item If someone becomes free, then add to end and change pointer
\item Since we never have more than \(n\) people free, can use \(\mod n\)
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}
\section{Lecture 5 \textit{<2017-09-21 Thu>}}
\label{sec:org4447901}
\subsection{Graphs}
\label{sec:org036c584}
\subsubsection{Undirected Graphs}
\label{sec:org0256ece}
\begin{itemize}
\item Notation \(G=(V,E)\)
\begin{itemize}
\item \(V =\) nodes (or vertices)
\item \(E =\) edges (or arcs) between pairs of nodes.
\item Captures pairwise relationship between object
\item Graph size parameters: \(n=|V|, m=|e|\)
\end{itemize}
\end{itemize}
\subsubsection{Example applications}
\label{sec:org3494bc7}
\begin{center}
\begin{tabular}{lll}
Graph & Node & Edge\\
\hline
Communication & telephone,computer & fiber optic cable\\
Circuit & gate, register, processor & wire\\
mechanical & joint & rod, beam, spring\\
financial & stock, currency & transactions\\
transportation & street intersection, airport & highway, airway route\\
internet & class C network & connection\\
game & board position & legal move\\
social relationship & person, actor & friendship, movie cast\\
neural network & neuron & synapse\\
protein network & protein & protein-protein interaction\\
molecule & atom & bond\\
\end{tabular}
\end{center}
\subsubsection{Ways of implementing in a program}
\label{sec:org74e5681}
\begin{enumerate}
\item Adjacency matrix
\label{sec:orgbad80e6}
\$n\$-by-\(n\) matrix with \(A_{uv} = 1\) if \((u,v)\) is an edge.
\begin{itemize}
\item Two representations of each edge.
\item Space proportional to \(n^2\)
\item Checking if \((u,v)\) is an edge takes \(\Theta(1)\) time
\item Identifying all edges takes \(\Theta(n^2)\) time
\item It's exactly symmetric
\end{itemize}
\item Adjacency list
\label{sec:org591a983}
Node-indexed array of lists
\begin{itemize}
\item Two representations of each edge
\item Space is \(\Theta(m+n)\)
\item Checking if \((u,v)\) is an edge takes \(O(degree(u))\) time
\item Identifying all edges takes \(\Theta(m+n)\) time
\end{itemize}
\end{enumerate}
\subsubsection{Paths and connectivity}
\label{sec:org51b6499}
\begin{itemize}
\item Def. A \textbf{path} in an undirected graph \(G=(V,E)\) is a sequence of nodes \(v_1,v_2,\ldots,v_k\) with the property that each consecutive pair \(v_{i-1},v_i\) is joined by an edge in \(E\).
\item Def. A path is \textbf{simple} if all nodes are distinct.
\item Def. An undirected graph is \textbf{connected} if for every pair of nodes \(u\) and \(v\), there is a path between \(u\) and \(v\)
\end{itemize}
\subsubsection{Cycles}
\label{sec:org3c1bf63}
\begin{itemize}
\item Def. A \textbf{cycle} is a path \(v_1, v_2, \ldots, v_k\) in which \(v_1 = v_k\), \(k>2\), and the first \(k-1\) nodes are all distinct.
\end{itemize}
\subsubsection{Trees}
\label{sec:org3a704ac}
\begin{itemize}
\item Def. An undirected graph is a \textbf{tree} if it is connected and does not contain a cycle
\end{itemize}
\begin{enumerate}
\item Theorem
\label{sec:orgf474146}
Let \(G\) be an undirected graph on \(n\) nodes. Any two of the following statements imply the third:
\begin{itemize}
\item \(G\) is connected
\item \(G\) does not contain a cycle
\item \(G\) has \(n-1\) edges
\end{itemize}
\item Rooted trees
\label{sec:org29f8680}
\begin{itemize}
\item Given a tree \(T\), choose a root node \(r\) and orient each edge away from \(r\).
\item Importance. Models hierarchical structure
\end{itemize}
\end{enumerate}
\subsubsection{Connectivity}
\label{sec:org64393d4}
\begin{itemize}
\item s-t connectivity problem. Given two nodes \(s\) and \(t\), is there a path between \(s\) and \(t\)?
\item s-t shortest path problem. Given two nodes \(s\) and \(t\), what is the length of a shortest path between \(s\) and \(t\)?
\item Applications.
\begin{itemize}
\item Friendster
\item Maze traversal
\item Kevin Bacon number
\item Fewest hops in a communication network
\end{itemize}
\end{itemize}
\subsubsection{Breadth-first search}
\label{sec:orga8d5897}
\begin{enumerate}
\item BFS intuition
\label{sec:org2b1eb22}
Explore outward from s in all possible directions, adding nodes one "layer" at a time. At most \(n\) layers.
\item BFS algorithm
\label{sec:org7503529}
\begin{itemize}
\item \(L_0=\{s\}\)
\item \(L_1 =\) all neighbors of \(L_0\)
\item \(L_2 =\) all nodes that do not belong to \(L_0\) or \(L_1\), and that have an edge to a node in \(L_1\)
\item \(L_{i+1}=\) all nodes that do not belong to an earlier layer, and that have an edge to a node in \(L_i\)
\end{itemize}
\item Theorem
\label{sec:orgc1ecabd}
For each \(i, L_i\) consists of all nodes at distance exactly \(i\) from \(s\). There is a path from \(s\) to \(t\) iff \(t\) appears in some layer.
\item Property
\label{sec:orgd929498}
Let \(T\) be a BFS tree of \(G=(V,E)\), and let \((x,y)\) be an edge of \(G\). Then, the levels of \(x\) and \(y\) differ by at most \(1\).

\item Analysis
\label{sec:orgfe02461}
\begin{enumerate}
\item Theorem
\label{sec:org8e2cbaf}
The above implementation of BFS runs in \(O(m+n)\) time if the graph is given by its adjacency representation.
\item Proof
\label{sec:orgf885de8}
\begin{itemize}
\item Easy to prove \(O(n^2)\) running time:
\begin{itemize}
\item At most \(n\) lists \(L[i]\)
\item Each node occurs on at most one list; for loop runs \(\leq n\) times
\item When we consider node \(u\), there are \(\leq n\) incident edges \((u,v)\), and we spend \(O(1)\) processing each edge
\end{itemize}
\item Actually runs in \(O(m+n)\) time:
\begin{itemize}
\item When we consider node \(u\), there are \(degree(u)\) incident edges \((u,v)\)
\item total time processing edges is \(\sum_{u\in V} degree(u)=2m\)
\end{itemize}
\end{itemize}
\end{enumerate}
\end{enumerate}
\section{Lecture 6 \textit{<2017-09-26 Tue>}}
\label{sec:org11c6a69}
\subsection{Stable Marriage}
\label{sec:org8305de7}
Continuation of Lecture 4: Stable Marriage algorithm analysis.
\begin{itemize}
\item Good data structure to tell if someone is free or not?
\begin{itemize}
\item Can have a linked list of all the free men, remove them when they're no longer free.
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i7.png}
\end{center}
Initially all men are here. Finding a free man: \(O(1)\)
\begin{itemize}
\item Keep an ordered list of women sorted according to \(m\)'s preference. Keep a pointer to the first person he has not proposed to yet. (can store as a linked list, array or stack) Move pointer along to the next after proposing.
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i8.png}
\end{center}

Now we need to know if the woman is free. Make a boolean array of women with true or false.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i9.png}
\end{center}
\begin{itemize}
\item Array telling whom \(w\) is engaged to (\(j^{th}\) entry contains who \(w_j\) is engaged to)
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i10.png}
\end{center}

\begin{itemize}
\item We keep a matrix \(w[i,j] =\) the rank of \(m_j\) in the eye of \(w_i\)
\item Example: \(w_2:m_4 > m_3 > m_5>m_2 \ldots\), \(w[2,5]=3\) (don't need to do linear time)
\item If \(w_i\) prefers \(m_j\) to \(mk\) \(\iff\) \(w[i,j]<w[i,k]\)
\begin{itemize}
\item Do some "preprocessing" in the beginning to make it easier during the algorithm
\end{itemize}
\end{itemize}

With all these data structures, our algorithm can run in \(O(n^2)\)
\subsection{Priority Queue}
\label{sec:orgb789753}
Say we're running a clinic and new patients come. A nurse assesses them and gives them a priority so that we know who we should see next. 

Dynamic Scenario
\begin{itemize}
\item Get elements with different priorities in an "online" matter (sometimes you get new data, not all given to you in the beginning)
\item Once in awhile we can serve the element with the highest priority (and remove from the set)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
We have a set \(S\).
\begin{itemize}
\item Initially \(S=\emptyset\)
\item At every step either
\begin{itemize}
\item A new number is added to \(S\).
\item or the smallest number is removed from \(S\).
\end{itemize}
\end{itemize}

Some ideas:
\begin{itemize}
\item An unsorted list:
\begin{itemize}
\item Inserting a new element \(O(1)\)
\item Removing the minimum: \(O(n)\) (\(n\) elements in the list, have to find smallest)
\item Too costly, not good.
\end{itemize}
\item Sorted list:
\begin{itemize}
\item Inserting a new element \(O(n)\)
\begin{itemize}
\item With an array, need to shift all elements.
\item Linked list (no binary search)
\end{itemize}
\item Removing the smallest \(O(1)\).
\end{itemize}
\end{itemize}
\subsubsection{Heap Data Structure}
\label{sec:org3d6b57c}
A balanced binary tree
\begin{itemize}
\item All levels are full except the last level which is filled \textbf{from left to right}
\item Every node is \(\geq\) its parent
\item Ex:
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i11.png}
\end{center}

Can be implemented with an array. Fill left to right.

\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i12.png}
\end{center}

Where are the children of entry \(i\)? \(2i, 2i+1\) (convenient)

\noindent\rule{\textwidth}{0.5pt}
What do we do when a new number arrives? Say \uline{insert(4)}
\begin{itemize}
\item Naturally we want to put it in the next available place "\(n^{th}\)" if \(n\) is the updated \# of nodes
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i13.png}
\end{center}

But 4 is smaller than its parent. How to fix? Swap with parent.

\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i14.png}
\end{center}

We will call this operation Heapify-Up.
\begin{algorithmic}
\State Heapify-Up$(H,i)$ // $i$ is index
\If {$i>1$} 
    \State let $j=parent(i)=\lfloor \frac{i}{2} \rfloor $
\If {$H[i]<H[j]$}
    \State swap$(H[i],H[j])$
    \State Heapify-Up$(H,j)$
\EndIf
\EndIf
\end{algorithmic}
Running time of Heapify-Up: \(O(\log n)=O(\text{Height of the tree})\)

\noindent\rule{\textwidth}{0.5pt}
How do we remove the minimum?
\begin{itemize}
\item Insert last element at head and then swap with smallest child until the tree is balanced
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i15.png}
\end{center}

\begin{algorithmic}
\State Heapify-down$(H,i)$
\State $n=$ length$(H)$
\If {$2i>n$} // Elements $> n/2$ have no children
    \State Terminate
\ElsIf {$2i+1 \leq n$}
       \State $left=2i, right = 2i+1$
       \If {$H[left]<H[right]$}
       	   \State $j=left$
	   \Else 
	   \State $j=right$
	   \EndIf
\Else //$(n=2i)$
      \State $j=left=2i$
\EndIf
\If {$H[j]<H[i]$}
    \State swap$(H[j],H[i])$
    \State Heapify-down$(H,j)$
\EndIf
\end{algorithmic}

\noindent\rule{\textwidth}{0.5pt}
Q: How can we use this data structure to sort a list of \(n\) numbers?

Answer: Insert the elements one by one and then extract the minimums one by one.
\begin{itemize}
\item Running time? \(2n O(\log n)\)
\begin{itemize}
\item \(O(n\log n)\)
\end{itemize}
\end{itemize}
\section{Lecture 7 \textit{<2017-09-28 Thu>}}
\label{sec:org180b015}
\subsection{Graph Exploration Algorithms}
\label{sec:org2193482}
\subsubsection{Breadth-First-Search (BFS)}
\label{sec:orgf688866}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i16.png}
\end{center}

Also tells you length of shortest path from s to any vertex.
\begin{itemize}
\item We explore according to the distance from s.
\item How to implement this?
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
\begin{algorithmic}
  \State BFS(G)
  \For {every vertex v in G}
  \If {v is unexplored}
  \State Mark v as explorerd
  \State BFS.vertex(v)
  \State connected-comp$++$
  \EndIf
  \EndFor
\end{algorithmic}

\noindent\rule{\textwidth}{0.5pt}
\begin{algorithmic}
  \State BFS-vertex(v)
  \State Make a list of all the unexplored neighbors of v.
  \State Mark every vertex in this list as explored
  \For {every u in this list}
  \State BFS-Vertex(u)
  \EndFor
\end{algorithmic}
Recursive way above does not work?

A good way to implement this is to keep the newly discovered vertices in a queue (FIFO, first in first out).
\begin{algorithmic}
  \State BFS-Vertex(v)
  \State Add v to the queue
  \While {queue is not empty}
        \State Pick the first vertex u in the queue.
        \State Mark all unexplored neighbors of u as explored and add
        them to the queue
  \EndWhile
\end{algorithmic}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i17.png}
\end{center}

\subsubsection{Depth-First-Search (DFS)}
\label{sec:org6b39c84}
We go in a path discovering new vertices until we reach a dead-end, and then we step back \(\ldots\)

\noindent\rule{\textwidth}{0.5pt}
\begin{algorithmic}
  \State DFS(u)
  \For {every edge (u,v)}
        \If{v is unexplored}
                \State{mark v as explored}
                \State{DFS(v)}
        \EndIf
  \EndFor        
\end{algorithmic}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i18.png}
\end{center}

\noindent\rule{\textwidth}{0.5pt}
Non-recursive DFS: Every time we discover a new vertex we put it at the top of a stack (FILO, first in last out).
\subsection{Data Structure for Graphs}
\label{sec:org57be51f}
What data structure to use for graphs?
\begin{itemize}
\item Adjacency Matrix
\begin{equation*}
  A[u,v] =
  \begin{cases}
    1 & \text{if }(u,v)\in E
    \\ 0 & \text{if }(u,v)\notin E
  \end{cases}
\end{equation*}
\begin{itemize}
\item Pros: very easy to see if u is connected to v
\item Cons: IF the graph has few edges it is wasteful. \(O(n^2)\) bits of memory.
\end{itemize}
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
\begin{itemize}
\item For every vertex v we keep a list of all edges (u,v) incident to v
\item Pros: easy to find the neighbors
\begin{itemize}
\item Doesn't take much memory if the graph is sparse
\end{itemize}
\item Cons: Takes \(O(n)\) to see if u is adjacent to v.
\end{itemize}
\subsection{Bipartites}
\label{sec:org53d9e99}
An undirected graph is called \uline{bipartite} if we can \uline{partition} the vetices into two parts \(R\) and \(B\) such that all the edges are between \(R\) and \(B\)
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i19.png}
\end{center}
\subsubsection{Testing for bipartites}
\label{sec:org760a455}
How can we test to see if \(G\) is bipartite? Label one vertex in \(R\) then:
\begin{itemize}
\item Look at neighbors to see if they're supposed to be \(R\) or \(B\)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i20.png}
\end{center}

\noindent\rule{\textwidth}{0.5pt}
\begin{algorithmic}
  \State DFS\_Bipartitite(G)
  \For {every vertex u in G}
        \If{u is not explored}
                \State color[u] = ``R''
                \State mark u as explored
                \State DFS(u)
        \EndIf
 \EndFor
 \If{not declared ``non-bipartite'' yet}
        \State declare ``bipartite''
\EndIf
      \end{algorithmic}

\noindent\rule{\textwidth}{0.5pt}
\begin{algorithmic}
  \For{each edge (u,v)}
  \If{v is not explored}
  \State Mark v as explored
  \State color v differently from color[u]
  \State DFS(v)
  \ElsIf{color[u]=color[v]}
  \State declare ``non-bipartite''
  \EndIf
  \EndFor
\end{algorithmic}
This is called proper two coloring of a graph.
\subsection{Directed Graphs}
\label{sec:org82577c8}
Every edge has an orientation.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i21.png}
\end{center}

\subsubsection{Data Structure:}
\label{sec:org9d3a7fd}
For every vertex keep two lists: the edge going out, the edges coming into that vertex
Given two vertices, s and t, is there a path from s to t?
\begin{itemize}
\item say s=a t=d
\item yes in the graph
\item But there is no path from d to a.
\end{itemize}
We can use the "directed" version of DFS to solve this problem: We run DFS(s) if t is explored then such a path exists otherwise it doesn't.

Def: A directed graph is called strongly connected if for every u and v there is a path from u to v. (can go from anywhere to anywhere)

\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i22.png}
\end{center}

\noindent\rule{\textwidth}{0.5pt}
Q: Given \(G\), how can we tell if it is strongly connected?
\begin{itemize}
\item Pick a vertex s
\item Run DFS(s) in \(G\)
\item If there is any unexplored vertex then "not strongly connected"
\item Run DFS(s) in \(G^{rev}\) (same as \(G\), but with directions reversed)
\item If \(\exists\) any unexplored vertex then "not strongly connected"
\item Otherwise declare "G is strongly connected"
\end{itemize}
\section{Lecture 8 \textit{<2017-10-03 Tue>}}
\label{sec:orge6537ea}
\subsection{Directed Graphs}
\label{sec:org2238bcd}
\begin{itemize}
\item Each edge has a direction (seen last class)
\item Not symmetric, edge from u to v means no edge from v to u.
\end{itemize}
\subsubsection{Graph search}
\label{sec:org782413c}
\begin{itemize}
\item Directed reachability
\begin{itemize}
\item Find all nodes reachable from a given node
\end{itemize}
\item Directed s-t shortest path problem
\begin{itemize}
\item Given two nodes, what is length of shortest path between them
\end{itemize}
\item BFS extends naturally to directed graphs
\item Web crawler
\begin{itemize}
\item Start from web page s. Find all web pages linked from s
\end{itemize}
\end{itemize}
\subsubsection{Strong Connectivity}
\label{sec:org47ac7a7}
\begin{itemize}
\item Node u and v are \textbf{mutually reachable} if there is a path from u to v and also a path from v to u.
\item A graph is \textbf{strongly connected} if every pair of nodes is mutually reachable.
\end{itemize}
\begin{enumerate}
\item Lemma
\label{sec:orga2c1a6d}
Let s be any node. G is strongly connected iff every node is reachable from s, and s is reachable from every node.
\begin{itemize}
\item Proof: \(\implies\) Follows from definition
\item \(\impliedby\) Path from u to v: concatenate u-s path with s-v path
\begin{itemize}
\item Path from v to u: concatenate v-s path with s-u path
\end{itemize}
\end{itemize}
\item Algorithm
\label{sec:org8d698cb}
\begin{enumerate}
\item Theorem
\label{sec:org72eada5}
Can determine if G is strongly connected in \(O(m+n)\) time.

Proof:
\begin{itemize}
\item Pick any node \(s\)
\item Run BFS from \(s\) in \(G\)
\item Run BFS from s in \(G^{rev}\)
\item Return true iff all nodes reached in both BFS executions
\item Correctness follows immediately from previous lemma
\item Has running time of BFS \(O(m+n)\)
\end{itemize}
\end{enumerate}
\end{enumerate}
\subsection{Directed Acyclic Graphs}
\label{sec:orgc0de911}
\begin{itemize}
\item A \textbf{DAG} is a directed graph that contains no directed cycles
\begin{itemize}
\item Good for modeling dependencies, like a course's prerequisites
\end{itemize}
\item Ex. Precedence constraints: edge \((v_i,v_j)\) means \(v_i\) must precede \(v_j\).
\begin{itemize}
\item Precedence constraints imply no cycle
\end{itemize}
\item A \textbf{topological order} of a directed graph \(G=(V,E)\) is an ordering of its nodes as \(v_1,v_2,\ldots,v_n\) so that for every edge \((v_i,v_j)\) we have \(i<j\)
\end{itemize}
\subsubsection{Lemma}
\label{sec:org06cb106}
If \(G\) has a topological order, the \(G\) is a DAG.

Proof (by contradiction)
\begin{itemize}
\item Suppose \(G\) has a topological order \(v_1,\ldots,v_n\) and that \(G\) also has a directed cycle \(C\).
\item Let \(v_i\) be the lowest-indexed node in \(C\) and let \(v_j\) be the node just before \(v_i\): thus \((v_j,v_i)\) is an edge.
\item By our choice of \(i\), we have \(i<j\)
\item On the other hand, since \((v_j,v_i)\) is an edge and \(v_1,v_2,\ldots,v_n\) is a topological order, we must have a contradiction. \lightning
\end{itemize}
\subsubsection{Lemma}
\label{sec:org8a204a5}
If \(G\) is a DAG, then \(G\) has a node with no incoming edges.

Proof (by contradiction)
\begin{itemize}
\item Suppose \(G\) is a DAG and every node has at least one incoming edge.
\item Pick any node \(v\), begin following edges backward from \(v\). Since \(v\) has at least one incoming edge \((u,v)\) we can walk backward to \(u\).
\item Since \(u\) has at least one incoming edge \((x,u)\) we can walk backward to \(x\)
\item Repeat until we visit a node, say \(w\), twice.
\item Let \(C\) denote the sequence of nodes encountered between successive visits to \(w\). \(C\) is a cycle. \lightning
\end{itemize}
\subsubsection{Lemma}
\label{sec:org8f7fc4d}
If \(G\) is a DAG, then \(G\) has a topological ordering.

Proof (by induction on \(n\))
\begin{itemize}
\item Base case: true if \(n=1\)
\item Given DAG on \(n>1\) nodes, find a node \(v\) with no incoming edges
\item \(G \setminus \{v\}\) is a DAG, since deleting \(v\) cannot create cycles
\item By inductive hypothesis, \(G\setminus\{v\}\) has a topological ordering.
\item Place \(v\) first in topological ordering: then append nodes of \(G\setminus \{v\}\) in topological order. This is valid since \(v\) has no incoming edges.
\end{itemize}
\begin{enumerate}
\item Algorithm
\label{sec:orgf7ce4f9}
To compute a topological ordering of \(G\)
\begin{itemize}
\item Find a node \(v\) with no incoming edges and order it first
\item Delete \(v\) from \(G\)
\item Recursively compute a topological ordering of \(G\setminus \{v\}\) and append this order after \(v\)
\item Running time: \(O(n)\) for each call, calling exactly \(n\) times. So algorithm runs in \(O(n^2)\). Lots of running time if the graph is sparse, not many edges. If we reimplement this more carefully, we can get \(O(m+n)\), with \(m\) being the number of edges. Note that making an algorithm run faster usually requires more space.`
\end{itemize}
\item Theorem
\label{sec:org01ebb3a}
Algorithm finds a topological order in \(O(m+n)\) time

Proof:
\begin{itemize}
\item Maintain the following information:
\begin{itemize}
\item \(count[w] =\) remaining number of incoming edges
\item \(S=\) set of remaining nodes with no incoming edges
\end{itemize}
\item Initialization: \(O(m+n)\) via single scan through graph.
\item Update: to delete \(v\)
\begin{itemize}
\item Remove \(v\) from \(S\)
\item Decrement \(count[w]\) for all edges from \(v\) to \(w\) and add \(w\) to \(S\) if \(count[w]\) hits \(0\)
\item This is \(O(1)\) per edge
\end{itemize}
\end{itemize}
\end{enumerate}
\section{Lecture 9 \textit{<2017-10-05 Thu>}}
\label{sec:org355a3a9}
\subsection{Greedy Algorithm}
\label{sec:org6a0d706}
\begin{itemize}
\item In every step, it tries to be myopic and optimize its current goal/step
\item Doesn't care about the future
\end{itemize}
\subsubsection{Interval scheduling}
\label{sec:org8b09c37}
\begin{itemize}
\item Have a class room and a microscope
\item Every request has a starting time and finishing time \(\{1,\ldots, n\}\), \((s_i,f_i)\)
\item Def. \(i\) and \(j\) are \uline{compatible} \((i+j)\) when \(f_i \leq s_j\) or \(f_j \leq s_i\)
\item Subset of requests is \uline{compatible} if every point of requests are compatible.
\item Maximum sized compatible subset is the \uline{optimal subset}
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
\begin{enumerate}
\item Pick \(s(i)\) with earliest request
\begin{itemize}
\item Might not give an optimal solution if the request that begins the earliest goes until the end, not allowing any of the other requests to be fulfilled.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i23.png}
\end{center}
\end{itemize}
\item \(f(i)-s(i)\) is the smallest 
\begin{itemize}
\item Can be problematic if the smallest is in between 2
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i24.png}
\end{center}
\item For each request compute the \# of requests it overlaps with. Pick the one with the smallest number.
\begin{itemize}
\item Still problematic
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i25.png}
\end{center}
\end{itemize}
\item \uline{Accept} (greedy rule) requests \(i\) for which \(f(i)\) is the smallest.
\begin{itemize}
\item Sort requests so that \(f(i_1)\leq f(i_2)\leq \ldots \leq f(i_n)\)
\item This one works
\end{itemize}
\end{enumerate}
\begin{algorithmic}
  \State $A = \emptyset$
  \For {$j=1$ to $n$}
  \If {j is compatible with $A$}
  \State $A \gets A \cup \{j\}$
  \EndIf
  \EndFor
  \State return A
\end{algorithmic}

\noindent\rule{\textwidth}{0.5pt}
\subsubsection{{\bfseries\sffamily TODO} clean up this section}
\label{sec:org79439bc}
Running time of method 4:
\begin{itemize}
\item Sort : \(O(n\log n)\)
\item \(f(j)\geq f(j^*) \forall i \in A, f(i) \leq f(j^*) \gets O(n)\)
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i26.png}
\end{center}
\end{itemize}
\subsubsection{Theorem}
\label{sec:org3dff165}
This greedy algorithm returns the optimal subset.
$$\underbrace{|A|}_{\text{optimal}} = |O| - \text{optimal subset}$$
\begin{itemize}
\item "stays ahead"
\item \(|A|=k, |O|=n\), assume \(k<m\)
\item \(O\) is ordered by their starting and finishing time for every \(j \in O\), \(f(i_1 \in A) \leq f(j)\)
\end{itemize}

\begin{enumerate}
\item Lemma.
\label{sec:orgdfedbb9}
For all \(r \leq k\), \(f(i_r) \leq f(j_r)\)
\begin{enumerate}
\item Proof
\label{sec:org47d0ae6}
\begin{itemize}
\item \(r=1 f(\underbrace{i_1}_A) \leq f(\underbrace{j_1}_O)\) Works
\item This greedy algorithm returns the optimal subset \(r-1\) i.e. \(f(i_{r-1})\leq f(j_{r-1})\).
\item But this contradicts \(f(i_r) \geq f(j_r) \implies f(i_r)\leq (j_r)\)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i27.png}
\end{center}


\begin{itemize}
\item \(A: i_1 \ldots i_k\)
\item \(O: j_1 \ldots j_k j_{k+1} \ldots\)
\item Apply the lemma with \(r=k\) so \(f(i_k)\leq f(j_k)\)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i28.png}
\end{center}
This contradicts \(k<m\)! Thus \(m=k\)
\begin{itemize}
\item Sort \(O\) by starting time, it's also sorted by finishing time
\end{itemize}
\end{enumerate}
\end{enumerate}
\subsubsection{Satisfying requests}
\label{sec:org562fdf6}
Given requests, how many resources do we need to satisfy all of them?
\begin{itemize}
\item Def. depth is the maximum number of requests that have a common point in the time line.
\end{itemize}
\begin{enumerate}
\item Claim
\label{sec:org8d5780e}
The \# of resources is at least \(d\). \$$\backslash${I\(_{\text{1}}\), \ldots , I\(_{\text{d}}\)$\backslash$}\$- requests with depth \(d\).
\end{enumerate}
\section{Lecture 10 \textit{<2017-10-10 Tue>}}
\label{sec:orgcde859f}
\subsection{Recall}
\label{sec:org047188d}
Interval scheduling
\begin{itemize}
\item Input: Lectures \(s_j, f_n\) (start and finish) \(j=1,\ldots n\)
\item Goal: Find the largest non-overlapping set.
\item Alg: Always pick the job with earliest finish time.
\end{itemize}
\subsection{Partition scheduling}
\label{sec:orgcbc2cd7}
Now we really want to accommodate all these jobs. How many rooms/resources do we need?
\begin{itemize}
\item Input: Same as above
\item Goal: Smallest number of rooms that can accommodate all the lectures.
\end{itemize}
\subsubsection{Greedy Template}
\label{sec:orgade9ccc}
Consider lectures in some \textbf{natural order}. Assign each lecture \uline{to an available room} (how?). If none is available open a new room.

\noindent\rule{\textwidth}{0.5pt}
Earliest-Start-Time-first
\begin{itemize}
\item (n, \(s_1\), \(\ldots\), \(s_n\), \(f_1\), \(\ldots\), \(f_n\))
\end{itemize}
Sort the lectures so that \(s_1 \leq s_2 \leq \ldots \leq s_n\)

\(d=0\) (number of rooms)
\begin{algorithmic}
  \For {j = 1,\ldots, n}
  \If {lecture $j$ is compatible with room $k$}
  \State Assign $j$ to room $k$
  \Else Assign $j$ to room $d+1$
  \State set $d=d+1$
  \EndIf
  \EndFor
\end{algorithmic}
Now that we have the algorithm, we need to analyze its correctness and its running time.
\begin{enumerate}
\item Running Time
\label{sec:org949eaf1}
\begin{itemize}
\item Sorting: \(O(n \log n)\)
\item For loop runs \(n\) times. Each time we check if a lecture is compatible with a room, so we must do this fast.
\item To see if lecture \(j\) is compatible with a room \(k\) we only need to compare \(s_j\) with the finishing time of the last lecture assigned to that room. (Since we know that none of the lectures in the room start after time \(s_j\))
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i29.png}
\end{center}
So for each room we keep a variable which tells us when the room becomes available.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i30.png}
\end{center}
We need to see if \(s_j > \min{(F_1,\ldots,F_d)}\). (How to do this quickly? Priority queue.)
\begin{algorithmic}
  \If {Yes} The room with minimum $F_k$ is available
  \Else \ Open a new room
  \EndIf
\end{algorithmic}
This is the priority queue problem: Always want to know the minimum (we can add or delete numbers from the list). Using a \uline{heap} this can be implemented so that all insertions and deletions can be done in \(O(\log n)\)

\noindent\rule{\textwidth}{0.5pt}
Running Time: 
\begin{itemize}
\item \(\underbrace{O(n \log n)}_{\text{Sort}} + \overbrace{n}^{\text{For loop}} \times \underbrace{O(\log n)}_{\text{Priority queue}} = O(n \log n)\)
\end{itemize}
\item Correctness
\label{sec:org421c621}
Why does this alg output the best solution?
\begin{itemize}
\item Depth: Max number of intervals that contain any point on the timeline
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i31.png}
\end{center}
\begin{itemize}
\item Obviously: Optimal \(\geq\) depth
\item Claim: When the algorithm opens a new room \(d\) then depth \(\geq\) d
\item Proof: Room \(d\) is opened since lecture \(j\) was incompatible with \(d-1\) other rooms.
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i32.png}
\end{center}
In this case every room \(1,\ldots,d-1\) has a lecture that ends after \(s_j\) (and starts before \(s_j\), due to the way the algorithm works). These together with \(j\) show depth \(\geq d\)

\noindent\rule{\textwidth}{0.5pt}
What do we know? depth \(\geq\) Output of algorithm d (just showed) \(\geq\) optimal \(\geq\) depth (earlier) \(\implies\) depth \(=\) optimal \(=\) output of alg
\end{enumerate}
\subsection{Minimizing Lateness}
\label{sec:orgf5244b0}
\begin{itemize}
\item Input: \(n\) tasks.
\begin{itemize}
\item Processing times: \(t_1, \ldots, t_n\)
\item Deadline: \(d_1,\ldots,d_n\)
\end{itemize}
\item Goal: We have a single processor. Ideally we want to schedule all tasks so that they all finish before their deadlines.
\end{itemize}
Each task will be scheduled for some time \(s_j = f_j-t_k\) to \(f_j\) (to finish at \(f_j\) we need to start at \(f_j-t_k\)).
\begin{itemize}
\item Lateness \(=\) \$\(\max_{\text{j}}\)\{f\(_{\text{j}}\) - d\(_{\text{j}}\)\} (Time we finish - deadline for job)
\item Goal: Minimize the lateness
\end{itemize}
\subsubsection{Greedy Template}
\label{sec:orgbf30f75}
Sort the jobs according to some order and assign them to the processor according to this order.

Shortest job first?
\begin{itemize}
\item This doesn't work.
\end{itemize}
\begin{center}
\begin{tabular}{rr}
Process Time & Deadline\\
\hline
1 & 100\\
10 & 10\\
\end{tabular}
\end{center}
Optimal is \(f=10, f=11\). But this alg gives us \(f=1, f=11\). 

Smallest slack \((d_j-t_j)\) first. But this might give us huge lateness.
\begin{center}
\begin{tabular}{rr}
t & d\\
\hline
1 & 2\\
10 & 10\\
\end{tabular}
\end{center}
Optimal: \(f=1, f=11\), lateness \(=\) 1

Alg: \(f=10, f=11\), lateness \(=\) 9
\subsubsection{Optimal Alg}
\label{sec:orgad93492}
Sort by the deadline: \(d_1 \leq d_2 \leq \ldots d_n\)
\begin{algorithmic}
  \State Set f $\gets 0$
  \For {$i=1 \ldots n$}
  \State Assign job $j$ to $[f,f+t_j]$
  \State $f=f+t_j$
  \EndFor
\end{algorithmic}
Running time: \(O(n \log n)\) (sort)

\noindent\rule{\textwidth}{0.5pt}
Why is this optimal? Suppose the optimal is not sorted according to deadlines. Then we will have \(i\) and \(j\): 

What will switching these two jobs do? It can only improve the lateness. 
\subsection{Midterm}
\label{sec:org7f1ad64}
Next class is the midterm, will be split into 2 rooms.
\begin{itemize}
\item Topics: Everything until today
\item Format: Similar to assignments, 3-4 questions like on the assignments
\item No crib sheets
\end{itemize}
\section{Lecture 11 \textit{<2017-10-17 Tue>}}
\label{sec:orgbd2a31b}
\begin{itemize}
\item Recall: Minimize Lateness
\begin{itemize}
\item Input: Jobs \((t_i, d_i), i=1,\ldots,n\), where \(t_i\) is the process time and \(d_i\) is the deadline.
\item In which order should we proceed them in order to \uline{minimize}
\item Lateness \(=max_i f_i - d_i\)
\begin{itemize}
\item Where \(f_i\) is the finishing time of job \(i\)
\end{itemize}
\end{itemize}
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Greedy alg: Process these jobs in increasing order of their deadlines
\begin{itemize}
\item (Earliest deadline first)
\item Sort \(d_1 \leq d_2 \leq \ldots \leq d_n\)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i33.png}
\end{center}

\noindent\rule{\textwidth}{0.5pt}
How do we show that this is optimal?
\begin{itemize}
\item Most greedy algorithm proofs are similar, start with the optimal solution and then show that the algorithm keeps with it
\end{itemize}
Consider an optimal solution. If different from the output of the algorithm (not sorted), then we can find 2 jobs that are not sorted in order of deadline
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i34.png}
\end{center}
\begin{itemize}
\item How does the lateness of the jobs we switch change?
\item \(f_i = T + t_i \rightarrow f'_i = T+t_i+t_j\)
\item \(f_j = T + t_i + t_j \rightarrow f'_j T + t_i\)
\item \(f'_j\) has better lateness than before (smaller lateness), but \(f'_i\) lateness might increase
\begin{itemize}
\item new lateness \(= T+t_i+t_j - d_i\)
\end{itemize}
\item Why won't this increase lateness? This is smaller than the original lateness of the \(j^{th}\) job \(= T+t_i+t_j- d_j\) (since \(d_i\) is larger than \(d_j\))
\end{itemize}
A different way of writing this proof: Among all optimal solutions pick the one that agrees with the greedy algorithm for the longest period.
\subsection{Optimal Caching}
\label{sec:org7d44b12}
(Very complicated)
\begin{itemize}
\item Cache with some capacity to store items
\item If someone requests an item that we have in the cache, we can show it to them
\item If they request something we don't have in the cache, then we have to remove it from the cache
\item Sequence of \(m\) requests: \(d_1, d_2, \ldots, d_m\)
\item Cache hit: The item is in the cache.
\item Cache miss: Item not in cache when requested. (Must bring the item to the cache and evict some existing item) This is a costly operation.
\item We want to make the cache optimal given the schedule beforehand
\item We assume that we start with a full cache.
\end{itemize}
Example: \(k=2\), initial cache |a|b|
\begin{itemize}
\item Requests:
\end{itemize}
\begin{center}
\begin{tabular}{lll}
 & cache & \\
\hline
1 \(\checkmark\) a & ab & \\
2 \(\checkmark\) b & ab & \\
3 miss \texttimes{}  c & cb & \(a \gets c\)\\
4 \(\checkmark\)   b & cb & \\
5 \(\checkmark\) c & cb & \\
6 miss \texttimes{} a & ab & \(c \gets a\)\\
7 \(\checkmark\) a & ab & \\
8 \(\checkmark\) b & ab & \\
\end{tabular}
\end{center}
We managed to do this one with \(2\) cache misses. How do we optimize this?

\noindent\rule{\textwidth}{0.5pt}
Greedy Alg: Evict the item that is needed farthest in the future. In the above example, in step 3, we see that a is needed in step 6 but b is needed in step 4, so we evict a.

Example: cache abc
\begin{tabular}{c | c | c}
  1 \checkmark a & abc
  \\ 2 \checkmark b & abc
  \\ 3 \checkmark c & ab\textbf{c}
  \\ 4 $\times$ d & abd & $c\gets d$
  \\ 5 \checkmark a & abd
  \\ 6 \checkmark d & a \textbf{b} d
  \\ 7 $\times$ e & aed & $b\gets e$
  \\ 8 \checkmark a & aed
  \\ 9 \checkmark d & aed
  \\ 10 $\times$ b & bed & $a\gets b$
  \\ 11 $\times$ c & ced & $b\gets c$
\end{tabular}
Can do anything for steps 10 \& 11, but are steps 4 and 7 unique? No, we can do \(b \gets d\) at step 4 instead. So the greedy algorithm is one solution, but it isn't the only solution, making it harder to prove.

\noindent\rule{\textwidth}{0.5pt}
Reminder: We will assume that we only evict items if there is a request that is not in the cache. (won't preemptively remove something)
\begin{itemize}
\item Read the book: There is no disadvantage in doing this
\end{itemize}

Proof: Among all the optimal solutions, pick the one that agrees with our algorithm for the longest period (assuming they all diverge eventually), call it solution S.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i35.png}
\end{center}
\begin{itemize}
\item From the algorithm, we know that e is requested earlier than f, say at step \(n\).
\item As for the optimal solution, at step \(n\) it must have e. Let \(t\) be the first time after \(j\) that S has \(g \gets e\) for some \(g\).
\begin{itemize}
\item \(t\) cannot be later than \(n\) so \(t \leq n\)
\item How do we satisfy \(t\) without increasing the number of cache misses in S and making the solution closer to our algorithm? Evict \(f\) at \(j\) instead of \(e\)
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i36.png}
\end{center}
So for the proof, either assume that there's an optimal solution that remains stays the same for \(j\) steps and reach a contradiction showing that it is the same for \(j+1\) steps or show that it keeps going on
\section{Lecture 12 \textit{<2017-10-19 Thu>}}
\label{sec:org43bc593}
\subsection{Shortest Path in Graphs}
\label{sec:org9c5af99}
\begin{itemize}
\item Input: Directed graph \(G=(V,E)\), source \(s\), destination \(t\)
\item \(\forall e, \ell e =\) length of edge \(e\)
\item Goal: Find the length of the shortest path from \uline{\(s\)} to \uline{\(t\)}.
\end{itemize}
\subsection{Dijkstra's Algorithm}
\label{sec:org394737e}
It will find shortest paths from \(s\) to all the other nodes in one go.
\begin{itemize}
\item Idea: We keep a list of all vertices (initially includes source)
\item We already know the lengths of the shortest paths from \(s\) to all the explored vertices
\item At the next step we choose the vertex with smallest
$$\pi(v)=\min_{\ell=(u,v)_\text{u is explored}} d(s,u)+\ell e$$
\end{itemize}
and mark that as explored and set \(d(s,v)=\pi(v)\)
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i37.png}
\end{center}

\noindent\rule{\textwidth}{0.5pt}
Alg: \(S =\) set of explored vertices
\begin{itemize}
\item \(d(u)=\) distance from \(s\) to \(u\) for explored \(u\)
\end{itemize}
\begin{algorithmic}
  \State set $S = \{s\}, d(s)=0$
  \While{$S \neq V$} choose $w \in V-S$ with minimum
  $\pi(w)=\min_{\ell=uw,u\in S}d(u)+\ell e$
  \State $S \gets S \cup \{w\}$
  \State $d(w)=\pi(w)$
  \EndWhile
\end{algorithmic}
\begin{itemize}
\item Example:
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i38.png}
\end{center}
\subsubsection{Correctness}
\label{sec:org90f88c5}
Claim: During the execution of the algorithm for every \(u \in S\), \(d(u)\) is the length of the shortest path from \(s\) to \(u\)
\begin{itemize}
\item Proof: We use induction on size of \(S\).
\begin{itemize}
\item Base: Trivial, \(S=\{s\}, d(s)=0\)
\item Induction Hypothesis: The claim remains true after adding next \(v\).
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i39.png}
\end{center}
\item If \(\pi(v)\) is not the length of the shortest \(s-v\) path
\begin{itemize}
\item Consider the shortest \(s-v\) path on the red path
\item Consider first vertex \(y\) outside \(S\) on the path. Let \(x\) be the previous vertex.
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i40.png}
\end{center}
\begin{itemize}
\item \(\pi (y)\leq d(x)+\ell xy \leq\) length of the red path \(<\pi(v)\) (because we assumed \(\pi(v)\) is not the shortest path from \(s\) to \(v\))
\item Contradiction as we assumed \(\pi(v)\) was the smallest (we want to pick smallest \(\pi\) outside of explored area and we showed that \(\pi(y)\) is clearly smaller)
\end{itemize}
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Runtime of implementation
\begin{algorithmic}
  \State set $S = \{s\}, d(s)=0$
  \While{$S \neq V$} choose $w \in V-S$ with minimum
  $\pi(w)=\min_{\ell=uw,u\in S}d(u)+\ell e$
  \State $S \gets S \cup \{w\}$
  \State $d(w)=\pi(w)$
  \EndWhile
\end{algorithmic}

\noindent\rule{\textwidth}{0.5pt}
\begin{itemize}
\item While: \(|V| = n\) iterations
\begin{itemize}
\item Computing \(\pi(w) \forall w \in V - S \rightarrow O(m) \rightarrow O(mn)\) after multiplying loop iterations
\begin{itemize}
\item Might be costly to calculate one \(\pi\), as we can have many incoming edges to a vertex, up to \(m\) incoming edges
\end{itemize}
\item Taking their min
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i41.png}
\end{center}
When we add \(v\) to \(S\) we onlny need to update the \(pi\) value for all \(w \in S-v\) with \(vw \in E\)
\begin{itemize}
\item If we use a binary heap to implement a priority queue for \(\pi\) values then
\item Finding \(\min \pi : O(\log n)\)
\begin{itemize}
\item (Extracting \(\min\) from a binary heap)
\end{itemize}
\item Updating the key \((\pi-value)\) for all \$w\(\in\) V-S with \(vw \in E\): Updating each one at these \(w\)'s costs \(O(\log n)\) (either heapify-up or heapify-down, depending on if we're increasing or lowering key)
\begin{itemize}
\item \(n \log n\) since a vertex might have linear amount of outward eges to unvisited vertices
\end{itemize}
\item Note that each edge \(vw \in E\) is causing at most one of those updates. It will never be visited again. Therefore total \# of these key updates is at most \(m=|E|\)
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i42.png}
\end{center}
So all these updates cost \(O(m \log n)\)
\begin{itemize}
\item Binary heap implementation \(O(m \log n + n \log n)\)
\item Fibonacci Heap: \(O(m+n\log n)\) (Won't be looking at this in this course as it's much more complicated)
\end{itemize}
\section{Lecture 13 \textit{<2017-10-24 Tue>}}
\label{sec:org2d2a5b5}
\subsection{The Minimum Spanning Tree Problems (MST)}
\label{sec:orgdd14bb6}
\begin{itemize}
\item Input: Undirected Connected Graph \(G=(V,E)\)
\begin{itemize}
\item To every edge \uline{e} a positive cost \(c_e >0\) is assigned
\end{itemize}
\item Goal: Find a spanning tree in \(G\) (i.e. a tree that includes all the vertices of \(G\)) with minimum cost.
\end{itemize}
$$cost = \sum_{\text{$e$ is an edge of the tree}} c_e$$
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i43.png}
\end{center}
$$cost = 4 + 6 + 5 + 8 + 11 + 9 + 7$$
\begin{itemize}
\item Why not check all the spanning trees? Very costly.
\item Cayley's Thm: Complete graph on \(n\) vertices have \(n^{n-2}\) spanning trees
\item So checking all the spanning trees requires exponential time \(\Omega(n^{n-2})\)
\end{itemize}
\subsubsection{Three Greedy Algorithms:}
\label{sec:org6444eea}
\begin{itemize}
\item \uline{Kruskal}: Start with \(T=\{\emptyset\}\). At each step add the edge with minimum cost that does not create a cycle until we find a spanning tree (i.e. \(n-1\) edges are added)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i44.png}
\end{center}
\begin{itemize}
\item \uline{Reverse Deletion}: Now start with \(T=E\) (all the edges). At every step we remove the most expensive edge from \(T\) that does not \uline{disconnect} it until we arrive at a spanning tree
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i45.png}
\end{center}
\begin{itemize}
\item \uline{Prims}: Start with a node \uline{s} (root) and greedily grow a tree from \uline{s} outward by adding the cheapest edge that leaves \(T\).
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i46.png}
\end{center}

\noindent\rule{\textwidth}{0.5pt}
\subsubsection{Correctness}
\label{sec:org33a22da}
Why do they all find the MST?
\begin{itemize}
\item Simplifying assumption: we assume that all \(c_e\) are different (just to simplify the presentation of the proof)
\end{itemize}
\uline{Cut Property}: Let \(S\) be a subset of nodes and \uline{\(e\)} be the minimum cost edge from \(S\) to \(\overline{S}\). Then \uline{\(e\)} is in every minimum spanning tree.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i47.png}
\end{center}
\uline{Proof}: Suppose not. Let \(T\) be an MST that does not include \uline{\(e\)}
\begin{itemize}
\item Consider the path that connects \(u\) to \(v\) in \(T\).
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i48.png}
\end{center}
Pick an edge on this path that goes from \(S\) to \(\overline{S}\) and replace it with \(e\). Thus way we find another spanning tree with smaller cost. This contradicts the assumption that \(T\) is a MST.
\begin{itemize}
\item Why am I allowed to do this? Why doesn't it create a cycle?
\begin{itemize}
\item Can this create a cycle? If adding \(e\) made a cycle, then we had a cycle in the original \(T\).
\item If there were two paths from \(u\) to \(v\) (such that adding \(e\) makes a cycle), then \(T\) already had a cycle.
\end{itemize}
\end{itemize}
Cycle Property: Let \(C\) be a cycle in \(G\) and let \(e\) be the most costly edge on this cycle. Then \(e\) does not belong to any spanning tree.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i49.png}
\end{center}
Proof: Suppose not. There is a MST "\(T\)" that contains \(e\).
\begin{itemize}
\item Remove \(e\) from \(T\). This will break \(T\) into two components \(S\) and \(\overline{S}\).
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i50.png}
\end{center}
Since \(C\) is a cycle it crosses the cycle at some other edge \(e'\). Adding \(e'\) instead of \(e\) creates a better spanning tree. A contradiction!

\noindent\rule{\textwidth}{0.5pt}
\uline{Prims}: Each time add the smallest edge from \(T\) to the rest of the graph (starting from a root \(s\)).
\begin{itemize}
\item Theorem: If all costs are different then there's a unique MST and Prims Alg finds it.
\item Pf: Consider a step of the alg. Let \(S\) be the component of the current \(T\).
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i51.png}
\end{center}
Prim's alg picks the smallest edge \uline{\(e\)} between \(S\) and \(\overline{S}\) and adds it to \(T\). By cut property \(e\) is in every MST. So our alg indeed only picks edges that are in every MST. This finishes` the proof of the theorem. 

\noindent\rule{\textwidth}{0.5pt}
Implementation: We maintain \(S\) (initially \(S=\empty\))
\begin{itemize}
\item For each \(v \notin S\) maintain an attachment cost.
\item \(a_v =\) The cost of the cheapest edge from \(S\) to \(v\)
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i52.png}
\end{center}
\item At each step we add the vertex with smallest attachment cost to \(S\) and \uline{update attachment costs}.
\item Using a priority queue seems like a good idea.
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Running time of updating attachment costs once \(v\) is added to \(S\). Only vertices in \(\overline{S}\) with edges to \(v\) need updates: There are \(\leq deg(v)\) of these vertices \(w\). If we use a binary heap to keep attachment costs then the updates have running time \(deg(v)\).
$$ dev(v) \times O(\log_n)$$
\begin{itemize}
\item \(\log_n\) for updating key in binary heap
\item So total running time:
\end{itemize}
$$ \sum_{v \in V}deg(v) )(\log n) = O(\log (n))\times \sum_{v \in V} deg(v) = O(m \log n)$$
\section{Lecture 14 \textit{<2017-10-26 Thu>}}
\label{sec:orgbee67c8}
\subsection{Recall}
\label{sec:orga86016e}
Minimum Spanning Tree Problem. (MST)
\begin{itemize}
\item Input: Undirected Graph \(G=(V,E)\)
\begin{itemize}
\item Every edge \uline{\(e\)} has weight \(c_e > 0\)
\end{itemize}
\item Goal: Find a \uline{spanning} tree in \(G\) with minimum weight
\item \uline{Cut Property}: If \(S,\overline{S}\) is a "cut" in \(G\) and \uline{\(e\)} is the cheapest edge between \(S\) and \(\overline{S}\) then \uline{\(e\)} is in every MST.
\begin{itemize}
\item Shows that Prims algorithm is optimal
\end{itemize}
\item \uline{Cycle Property}: IF \(e\) has the max cost edge in a cycle \(C\) then \(e\) is not in any MST.
\end{itemize}
\subsection{The Kruskal Alg (Proof of Correctness)}
\label{sec:org78ca2ed}
Sort the edges from lowest cost to highest.
\begin{itemize}
\item Add these edges one by one to the tree (skipping the ones that create a cycle)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i53.png}
\end{center}
\subsubsection{Thm:}
\label{sec:orgecba5f9}
If costs are different then the Kruskal Algorithm finds the (unique) minimum spanning tree.
\subsubsection{Pf:}
\label{sec:org9eff3c3}
(Let's look at the edges that the algorithm skips and what we can say about those edges)
\begin{itemize}
\item Note that the edges are added in the increasing order according to their costs.
\item Let's consider the point in the execution of the algorithm where we are deciding whether to include \uline{\(e\)} or to skip it.
\item We know that the edges included so far are all cheaper than \uline{\(e\)}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i54.png}
\end{center}
We will skip \uline{\(e\)} only if \(e\) creates a cycle with the currently included edges. In this case \uline{\(e\)} is the most expensive edge in that cycle and thus by the cycle property, has to be excluded. This shows that all the excluded edges do not belong to any MST. So the included edges form the unique MST. (In contrast with Prims, here we're showing that every edge we exclude has to be excluded, whereas in the proof for Prims we showed that every edge added had to be added)
\subsection{Application}
\label{sec:orgc2c1ba0}
A clustering problem. (Analyzing data, saying what is similar)

\uline{\$k\$-clustering problem}: Given \(n\) points with pairwise distances \(d(i,j)=d(j,i)\geq 0\) (distance between \(i\) and \(j\))
\begin{itemize}
\item \(d(i,j) = 0 \iff i=j\)
\item Not necessarily geometric distances, may be similarities
\begin{itemize}
\item Does not satisfy triangle inequality (or else it would be a metric)
\end{itemize}
\item Input: \(d(i,j) \ \forall i,j\), a para
\end{itemize}
meter \(k \in \mathbb{N}\)
\begin{itemize}
\item Goal: "Partition" the set of points into \(k\) sets so that the "spacing" is maximized where
\begin{itemize}
\item spacing = \(\min_{i,j \\ \text{in different clusters}} d(i,j)\)
\item Ex. \(k=3\)
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i55.png}
\end{center}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i56.png}
\end{center}
Want minimum distance between clusters to be big, or else you're saying two points are different even though they're quite close/similar to each other.
\subsubsection{Algorithm}
\label{sec:org4dd5d32}
An algorithm for this: 
\begin{itemize}
\item Remark: Note that the "worst" clustering puts the closest two points in different clusters.
\item Now among all clusterings that put the two closest points in the same cluster, which one is the worst?
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i57.png}
\end{center}
\begin{itemize}
\item Answer: Any clustering that separates the next pair of closest points
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i58.png}
\end{center}

\noindent\rule{\textwidth}{0.5pt}
We are basically running the Kruskal algorithm until we have \(k\) connected components and then we stop. Connected components are the desired clusters. Here costs are the distances. 
\subsubsection{Remark}
\label{sec:orgbda72e0}
Another way to think about this algorithm is that "we find the MST and then remove the \((k-1)\) most expensive edges
\subsubsection{Thm}
\label{sec:org98c5feb}
Let \(C^{*}\) denote the clustering \(C_1^*,\ldots C_k^{*}\) obtained by deleting the \((k-1)\) most expensive edges from the MST. Then \(C^{*}\) is the \$k\$-cluster with largest spacing,
\subsubsection{Proof}
\label{sec:org3b1b319}
Let \(C_1,\ldots,C_k\) be a different \$k\$-clustering.
\begin{itemize}
\item Let \(i\) and \(j\) be two points that are in the same cluster in  \(C^{*}\) but in different clusters in \(C\).
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i59.png}
\end{center}
$$spacing \leq \ d(i,n)$$
\begin{itemize}
\item It suffices to show \(d(i,j)<spacing  \ of C^*\)
\end{itemize}
\section{Lecture 15 \textit{<2017-10-31 Tue>}}
\label{sec:org4d5ac95}
\subsection{Data Compression/Huffman Codes}
\label{sec:org65f262a}
\begin{itemize}
\item Very essential and important result in coding theory
\item Wouldn't have the digital world without it
\item Trying to compress a file that consists of characters. Want to make it into a smaller file, optimize it. How small can you make it? How redundant is it? Is it the same character over and over or random characters?
\end{itemize}

Question: Consider a file that uses certain characters (say 32 characters). How can we encode this in bits?
\begin{itemize}
\item The easiest way is to assign a unique 5-bit string to each one of these characters (There are \(2^{5}=32\) such strings)
\item Example: \(c(a)=00000, c(b)=00001, \ldots , c(z)=11001, \ldots\)
\end{itemize}
Now given a string of characters we can "code" it and easily "decode" it back
\begin{itemize}
\item \(abb \stackrel{\text{code}}{\rightarrow} 000000000100001\)
\item \(00001,00000 \stackrel{\text{decode}{\rightarrow}} ba\)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
\uline{Efficiency}: Suppose that some letters in the file are way more frequent than the others
\begin{itemize}
\item In this case it is more efficient to assign fewer number of bits to frequent characters and more to others
\item Doing this in an arbitrary way can cause a problem!
\begin{itemize}
\item Say \(c(a)=1, c(b)=01, c(c)=010\)
\item Consider \(0101\). How can we decode this? 01,01 -> bb, 010,1 -> ca
\begin{itemize}
\item Won't know original string here, ambiguity
\item Didn't face this problem in the original approach
\end{itemize}
\end{itemize}
\item How can we avoid this?
\begin{itemize}
\item What property of the code words can guarantee that we won't run into this problem?
\item We do not want any code word to be a prefix of another one.
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i60.png}
\end{center}
cw1 is a prefix of cw2.
\begin{itemize}
\item Example \uline{01101} is a prefix of \uline{0011010010}
\end{itemize}
\subsubsection{Prefix property}
\label{sec:org24cce3f}
\uline{Def}: We say that an encoding has \uline{prefix property} if no code word is a prefix of another
\begin{itemize}
\item Ex: \(c(a)=0, c(b)=10, c(c)=11\) has prefix property
\item Ex: \(c(a)=0, c(b)=10, c(c)=110, c(d)=111\) has prefix property
\item When you make one code short (like a), you're paying by making the others longer
\end{itemize}
For this example
\begin{itemize}
\item \(001011110110\)
\item \(0,0,10,111,10,110\)
\item a,a,b,d,b,c
\end{itemize}
\subsubsection{Prefix Codes as binary trees}
\label{sec:org7cbe071}
\begin{itemize}
\item The two codes in the previous two  examples can be represented by the leaves of the following trees
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i61.png}
\end{center}
This has prefix property because all the codes are only on leaves

Given a binary tree let "Left" be \uline{\(0\)} and "Right" be \uline{\(1\)}
\begin{itemize}
\item Now every path from the root to a leaf corresponds to a zero-one string and these strings have the prefix property as the prefix of a code leads to an internal node and not a leaf
\end{itemize}
\subsubsection{The best prefix codes}
\label{sec:orga0e0934}
Consider the text and assume that for every letter \(x\), \(f_x\) is its frequency. 
$$ f_x = \frac{\text{\# of times $x$ appears in text}}{\text{total \# of characters in text}}$$
Example: \(abbbacddaa\)
\begin{itemize}
\item \(f_a = \frac{4}{10}=0.4\)
\item \(f_b = \frac{3}{10}=0.3\)
\item \(f_c = \frac{1}{10}=0.1\)
\item \(f_d = \frac{2}{10}=0.2\)
\end{itemize}
\uline{Def}: Average bit per letter of a prefix code is
$$ ABL(c) = \sum_x f_x \cdot |c(x)| = \frac{\text{\# of bits in the coded version}}{\text{Size of the original text}}$$
\uline{Goal}: Minimize \# bits in the coded version. Equivalently minimize \(ABL(c)\)

\noindent\rule{\textwidth}{0.5pt}
Observation: In the binary tree representation, need as many bits as depth of the leaf to represent that number. So
$$ ABL(c) = \sum_{leaves\ x}f_x \cdot depth(x)$$
\uline{Def}: A binary tree is called \textbf{full} if every node has \uline{\(0\)} or \uline{\(2\)} children.
\begin{itemize}
\item \uline{Claim}: The optimal prefix code has a full binary tree.
\item \uline{Proof}: If tree is not full then we can improve it in the following way.
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i62.png}
\end{center}
We have the same set of leaves and the depth of some creases so
\(ABL(c) = \sum f_x depth(x)\) has decreased. So the initial tree was not optimal.

How do we come up with the codes? Want more frequent characters to have less bits, put higher up on the tree.
\begin{itemize}
\item \uline{Observation}: Since we are trying to minimize \(\sum f_x \cdot depth(x)\) We want to assign letters with larger frequencies to leaves at lower depths and letters with smaller frequencies to leaves at higher depths.
\item Claim: There is an optimal in which the two least frequent letters are at the highest depth and are siblings.
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i63.png}
\end{center}
\begin{itemize}
\item If one of them was higher up (lower depth) then we could have swapped it with a more frequent letter at higher depth and decreasing \(ABL(c)\).
\item Example: \(f_z =0.1, f_a = 0.2\), \(z\) (least frequent) is on the \(3^{rd}\) layer, then swapping it from \(2\) to \(5\) gives you:
\end{itemize}
$$ 0.1 \times 3 + 0.2 \times 5 \implies 0.1 \times 5 + 0.2 \times 3$$
\begin{itemize}
\item Shuffling letters at the same depth does not change \(ABL(c)\). So we can bring the least frequent \# letters next to each other
\end{itemize}
\subsubsection{Huffman Coding with an example}
\label{sec:org9e595ca}
\begin{itemize}
\item \(f_a =0.4, f_b = 0.3, f_c = 0.1, f_d = 0.2\)
\item At each step we take the two least frequent nodes and make them the children of a new node and assign the summation of the frequencies to this new word. (Bottom up construction)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i64.png}
\end{center}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i65.png}
\end{center}`
\section{Lecture 16 \textit{<2017-11-02 Thu>}}
\label{sec:orgeb1c683}
\subsection{Recall: Huffman Coding}
\label{sec:org190b7dc}
\begin{itemize}
\item Frequencies: \(f_x = \frac{\# \text{ appearances of }x}{\# \text{ of characters}}\)
\item Goal: Find the best prefix code. Minimize the number of bits. Equivalently minimize
\end{itemize}
$$ABL(c)=\sum_{x}f_x = |c(x)|=\frac{\#\text{ bits}}{\#\text{ of characters}}$$
\begin{itemize}
\item Idea was to see how frequent strings are and to replace frequent strings with shorter codes and rarer strings with longer codes
\item With prefix codes they're easy to decode, no ambiguity
\begin{itemize}
\item When you see a code word you know that you're not looking at any other code word
\end{itemize}
\end{itemize}
\subsubsection{Observation 1:}
\label{sec:org47851d2}
The optimal binary tree is full. i.e. every node has zero or two children
\subsubsection{Observation 2:}
\label{sec:org90e0d56}
The two least frequent letters appear at max depth and we can assume that they are siblings (if they aren't you can make them siblings without making your code worse).
\subsection{Huffman Coding}
\label{sec:orgc4ab10a}
At each step take the two least frequent nodes and make them siblings by creating a new node as their parent and assign the sum of the frequencies to the parent.
\begin{itemize}
\item \(f_a=0.1, f_b=0.1, f_c=0.3, f_d=0.5\)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i66.png}
\end{center}
\subsubsection{Thm}
\label{sec:org851cb19}
Huffman code has the optimal ABL among all prefix codes.
\subsubsection{Proof}
\label{sec:org169ccdb}
Let's consider an optimal code \uline{c} that satisfies the properties mentioned in observations I and II.

The proof is by induction.
\begin{itemize}
\item Base case: Only one letter. In this case the best we can do is to assign a 1-bit string to this letter and that is what Huffman Code does.
\item I.H. Huffman code is optimal if we have \uline{\(m\)} letters.
\item I. Step: We want to show that Huffman Code is optimal for \(m+1\) letters
\end{itemize}
Let \(c\) be an optimal code as described above. Consider the tree of \(c\):
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i67.png}
\end{center}
The two least frequent letters \(a,b\) are as in the picture.
\begin{itemize}
\item Consider the same text but replace occurrences of both \(a\) and \(b\) with a new character [ab]. The new next has \(m\) characters.
\item ex: a b c c c a b -> [ab] [ab] c c c [a] [b]
\end{itemize}
Let's rename the leaves \(a,b\) from the optimal tree and assign [ab] to their parent (now a leaf)
\begin{itemize}
\item Call the new code c'
\end{itemize}
$$ABL(v)=\sum_x f_x \times |c(x)| vs ABL(c')=\sum_x f_x \times |c'(x)|$$
\begin{itemize}
\item So \(c(x)=c'(x)\) for all abl characters except \(a,b\)
\end{itemize}
$$|c'([ab])|=|c(a)|-1=|c(b)|-1$$
$$ABL(c')=ABL(c)-f_a-f_b$$
By induction hypothesis the Huffman coding applied to the new text (the one [ab] character) leads to a code with
$$ABL \leq ABL(c')$$
Now we compare Huffman coding of the new text to the old text:
\begin{itemize}
\item Huffman code of new file:
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i68.png}
\end{center}
\begin{itemize}
\item Adding the red part gives us the Huffman tree for the original text.
\end{itemize}
$$ABL(\text{Huffman original})=ABL(\text{Huffman for the []})+f_a+f_b$$
I.H. \(ABL(\text{Huffman for []}) \leq ABL(c')\)
\begin{itemize}
\item (Showed earlier:) \(ABL(c')=ABL(c)+f_a+f_b \rightarrow ABL(\text{Huffman original})\leq ABL(c)\)
\end{itemize}
\subsection{Divide and Conquer}
\label{sec:orgc4bb433}
\begin{itemize}
\item Break up the input into several parts.
\item Solve each part \uline{recursively}.
\item Combine the solution to sub-problem into a solution for the original problem
\end{itemize}
\subsubsection{Example: Merge Sort}
\label{sec:org0ac39d5}
\begin{itemize}
\item Divide the array into two equal parts.
\item Sort each part recursively
\item Merge the two parts into one sorted array.
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Sort the letters of ALGORITHMS
\begin{itemize}
\item ALGOR | ITHMS, Divide \(O(1)\)
\item AGLOR | HIMS, recursive and sort params \(2T(n/2)\)
\item AGHILMORST, merge \(O(n)\)
\end{itemize}
\(T(n)=2T(n/2)+O(n)\), recursive formula, how fast is it really?
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i69.png}
\end{center}
\begin{itemize}
\item Merging cost = \(n \log n\)
\item Running time = \(O(n\log n +n)=O(n \log n)\)
More formal way to prove using induction:
\end{itemize}
\begin{enumerate}
\item Thm
\label{sec:orgc3788ff}
\begin{equation*} T(n) \leq
\begin{cases}
2T(n/2)+cn & \text{if }n>1
\\ c & \text{if }n=1
\end{cases}
\end{equation*}
Then \(T(n)\leq c n\log_2 n\) (\(n\) is power of \(2\)).
\item Pf.
\label{sec:orgaf35c81}
Assume \(n=2^m\), \(m\in \mathbb{N}\cup\{0\}\)
\begin{itemize}
\item We use induction on \(m\).
\item Base: \(m=0: T(2^0)=T(1)\leq c\)
\item I.H. \(T(2^m)\leq c2^m \log 2^m = c2^m m\)
\item I. Step: \(\underbrace{T(2^{m+1})}_n\leq \underbrace{2T(2^h)}_{n/2}+\underbrace{c2^{m+1}}_n \stackrel{I.H}{\leq} 2 c2^m m+c2^{m+1}=c2^{m+1}m+c2^{m+1}=c2^{m+1}(m_1)=cn\log n\)
\end{itemize}
\end{enumerate}
\section{Lecture 17 \textit{<2017-11-07 Tue>}}
\label{sec:org96c5e2f}
\subsection{Recall: Merge sort}
\label{sec:orgea045bc}
\begin{itemize}
\item \(T(n)=2T(n/2)+O(n)(\text{n from merging})=\Theta(n \log n)\)
\end{itemize}
\subsection{Counting Inversions}
\label{sec:orge7efa88}
\begin{itemize}
\item Input: A sequence of \(n\) distinct numbers \(a_1,a_2,\ldots,a_n\)
\item Output: How many \(i<j\) satisfy \(a_i>a_j\) (number of pairs that are in the wrong order)?
\end{itemize}
\subsubsection{Example}
\label{sec:orgb1e533c}
Array: 5, 2, 1, 4, 3
\begin{itemize}
\item (5,2)
\item (5,1)
\item (5,4)
\item (5,3)
\item (2,1)
\item (4,3)
\item \(6\) inversions
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
1,2,3,4,5 has \(0\) inversions
\subsubsection{Brute-Force}
\label{sec:org91e040a}
\begin{algorithmic}
\For{$i=1,\ldots,n$}
	\For{$j=i+1,\ldots,n$}
		\If{$a_i>a_j$} counter++
		\EndIf
	\EndFor
\EndFor
\end{algorithmic}
This is \(\Theta(n^2)\).
\subsubsection{Divide and Conquer}
\label{sec:orgaab385d}
\begin{itemize}
\item Divide: Separate the sequence into two halves \(O(1)\)
\item Conquer: Count the inversions in each part recursively \(2T(\frac{n}{2})\) (solving 2 halves)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i70.png}
\end{center}
\begin{itemize}
\item Combine: Counting number of inversions where \uline{\(i\)} and \uline{\(j\)} are in different halves
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i71.png}
\end{center}
\end{itemize}
Naive Combine: 

\begin{algorithmic}
\For{$i=1,\ldots,\frac{n}{2}$}
	\For{$j=\frac{n}{2}+1,\ldots,n$}
		\If{$a_i>a_j$} counter++
		\EndIf
	\EndFor
\EndFor
\end{algorithmic}
This is \(\Theta(\left(\frac{n}{2}\right)) = \Theta(n^2)\), bad.

\begin{itemize}
\item \(T(n)=2T(\frac{n}{2})+\Theta(n^2)\)
\item \(T(n)=\Theta(n^2)\), no improvement over brute force
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Idea: Suppose each part is sorted. Then combine can be done faster. 
-Example
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i72.png}
\end{center}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i73.png}
\end{center}
\(O(n)\) if they were sorted.

\noindent\rule{\textwidth}{0.5pt}
Key Observation: Sorting the two halves does not change the number of inversions in the combine step.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i74.png}
\end{center}
\begin{itemize}
\item If \((i,j)\) is an inversion as in the picture, \((a_i, a_j)\) will remain one.
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Attempt: 
\begin{itemize}
\item Divide into two halves \(O(1)\)
\item Conquer recursively \(2T(\frac{n}{2})\)
\item Sort \((O(n\log n))\)
\item Combine \(O(n)\)
\end{itemize}
\(T(n)=2T(\frac{n}{2})+O(n \log n)=O(n \log^2 n)\). Want to get rid of \(n \log n\)

\noindent\rule{\textwidth}{0.5pt}
Final Alg:
\begin{itemize}
\item Divide into two halves \(O(1)\)
\item Conquer: "Sort" and "count inversions"
\item Combine: "Merge" \(O(n)\) the sorted arrays and count inversions \(O(n)\)
\end{itemize}
Running Time:
\begin{itemize}
\item \(T(n)=2(\frac{n}{2})+O(n)\)
\item \(T(n)=O(n \log n)\)
\end{itemize}
\subsubsection{Applications}
\label{sec:org516bafe}
Closest Pair of Points
\begin{itemize}
\item Input: The coordinates of \(n\) points on the plane
\item Goal: Find a pair with smallest distance between them
\end{itemize}
Brute-force, \(O(n^2)\)
\begin{algorithmic}
\For{$i=1,\ldots,n$}
    \For{$j=i+1,\ldots,n$}
	\If{$dis(P_i,P_j)<min$} update min
	\EndIf
    \EndFor
\EndFor
\end{algorithmic}
Divide-and-conquer:
Find a vertical line that separates the points into two equal size halves.
Assumption: Points have distinct \(x\) coordinates. 
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i75.png}
\end{center}
(sort the points according to their \(x\) coordinates and find the middle point, \$O(n\(\log\) n))

Conquer: Find the closest pair in each part recursively. \(2T(\frac{n}{2})\)
Combine: Find the closes pair with each pair on one side of the line. Compare this to the pairs found in "conquer step" (distance between 2 sides we split). Return the best of the three.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i76.png}
\end{center}
Finding the closest pair with points in different parts: 
\begin{itemize}
\item Naive approach: \(\frac{n}{2}+\frac{n}{2}\)
\item \(T(n)=2T(\frac{n}{2})+\Theta(n^2) = \Theta(n^2)\), no improvement over brute-force.
\end{itemize}
Observation: If \(\delta\) is the best distance found in the recursion, we only need to consider a window of width \(2\delta\) around the splitting line. Unfortunately all or a significant number of points can be in this \$2\(\delta\)\$-strip.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i77.png}
\end{center}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i78.png}
\end{center}
Suppose that the points are sorted according to their \(y-coordinates\). Now for the point \(P_i\) (blue point in picture) in the \(2\delta-strip\)
\begin{itemize}
\item Want to find \(P_i, P_j\) with
\begin{itemize}
\item \(P_i\) one side \(+ dist(P_i,P_j)\)
\item \(P_j\) other side
\end{itemize}
\end{itemize}
Claim: We only need to look at \(j \in [i-12, i+12]\)
\begin{itemize}
\item Sort the points according to y-coord
\end{itemize}
\begin{algorithmic}
\State $min=\delta$
\For{$i=1,\ldots,n$}
    \For{$j=i-20$ to $i+20$}
	\If{$P_i,P_j$ are on different sides and $dist(P_i,P_j)<min$} update min
	\EndIf
    \EndFor
\EndFor
\end{algorithmic}

Running time: \(O(n log n)\) sort, \(O(n)\) nested for loop gives you: \(T(n)=2T(n/2)+O(n log n) = \Theta(n^2\log n)\)

Exercise, can show \(T(n)=2T(n/2)+O(n)=\Theta(n \log n)\)
\section{Lecture 18 \textit{<2017-11-09 Thu>}}
\label{sec:orga89ecf0}
\subsection{Integer Addition}
\label{sec:org0ba02bf}
So far we've considered these basic operations as constant operations, like adding two numbers, comparing two numbers, etc. But they really aren't, since they're sequence of bits.
\begin{itemize}
\item Input: Two \(n-bit\) positive integers \(a,b\)
\item Output: \(a+b\), \(n+1-bit\) number.
\item Example:
\end{itemize}
\begin{flalign*}
& 11010101 &
\\ + & 01111101
\\ \hline
\\ 1&01010010
\end{flalign*}
Basically need a for-loop to keep track and check if there's a carry over. This is \(O(n)\). Cannot be improved as even reading the input requires \(\Omega(n)\).
\subsection{Integer Multiplication}
\label{sec:orge14e61b}
\begin{itemize}
\item Input: Two \(n-bit\) integers \(a,b>0\)
\item Goal: Output \(a \times b\)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
\begin{flalign*}
& 0101 &
\\ \times & 1101
\\ \hline
\\ & 0101
\\ 0&0000
\\ 01&0100
\\ +010&1000
\\ \hline
\\ 100&0001
\end{flalign*}
Running time: \((n-1)\) additions of numbers with at most \(2n\) bits \(\implies O(n^2)\). Can we improve this? Yes.
\subsubsection{Divide and Conquer approach}
\label{sec:orgd402816}
Split each one of \(a,b\) into two \(\frac{n}{2}-bit\) (assume \(n\) even) numbers.
\begin{itemize}
\item \(a=2^{n/2}a_1+a_0\)
\item \(b=2^{n/2}b_1+b_0\)
\item Example: \(a = \underbrace{01}_{a_1}\underbrace{01}_{a_0}\)
\begin{itemize}
\item \(b = \underbrace{11}_{b_1}\underbrace{01}_{b_0}\)
\end{itemize}
\end{itemize}
\begin{flalign*}
ab = 2^n a_1 b_1 + 2^{n/2}(a_1b_0+b_1a_0)+a_0b_0
\end{flalign*}
Recursively compute: \(a_0b_0\)
\begin{itemize}
\item \(a_1b_0\)
\item \(a_0b_1\)
\item \(a_1b_1\)
\item \(4T(n/2)\)
\end{itemize}
Same shifting: \(2^na_1b_1, \ldots\), \(O(n)\)

Same additions: Three of these. \(O(n)\)

\(T(n)=4T(n/2)+O(n)\)
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i79.png}
\end{center}
\begin{itemize}
\item Number of leaves \(4^{\log_2 n}=n^2\)
\end{itemize}
Expanding this
$$T(n)=\underbrace{T(1)+\ldots+T(1)}_{n^2}+O(n)+4O(\frac{n}{2})+\ldots \implies T(n)=\Theta(n^2)$$
\begin{itemize}
\item So no improvement from before.
\end{itemize}
Or by using Master Theorem:
$$\log_2 4 > 1 \implies O(n^{\log_2 4}) = O(n^2)$$

\noindent\rule{\textwidth}{0.5pt}
Karatsuba 1962
\begin{itemize}
\item \(a=2^{n/2}a_1+a_0\)
\item \(b=2^{n/2}b_1+b_0\)
\item \(ab=2^na_1b_1+2^{n/2}((a_0+a_1)(b_0+b_1)-a_1b_1-a_0b_0)+a_0b_0\)
\begin{itemize}
\item Different way of writing \(ab\) in the previous approach
\end{itemize}
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Karatsuba's Alg:
\begin{itemize}
\item Recursively compute:
\begin{itemize}
\item \(a_1b_1,a_0b_0,(a_0+a_1)\times(b_0+b_1)\)
\item This time we only have 3 multiplications instead of 4. The rest is addition and shifting, which costs \(O(n)\).
\end{itemize}
\end{itemize}
$$T(n)=3T(n/2)+O(n)$$
Applying Master's Theorem: 
$$ \log_2 3 > 1 \implies \Theta(n^{\log_2 3}) = \Theta(n^{1.585\ldots})$$
\begin{itemize}
\item This algorithm isn't used in practice and can be improved to \(O(n \log n \log \log n)\) (Fast Fourier transforms)
\end{itemize}
\subsection{Matrix Multiplication}
\label{sec:orgfef790e}
\begin{itemize}
\item Input: Two \(n\times n\) matrices. \(A,B\)
\item Output: \(A \times B\)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Simple Alg: \(C=AB\)
$$C_{ij}=\sum_{k=1}^{n}A_{ik}B_{kj}$$
\begin{algorithmic}
\State $C=[0]_{n\times n}$
\For{$i=1,\ldots,n$}
	\For{$j=1,\ldots,n$}
		\For{$k=1,\ldots,n$}
			\State $C_{ij}+=A_{ik}B_{kj}$
		\EndFor
	\EndFor
\EndFor
\end{algorithmic}
This is \(O(n^3)\) if we assume that the matrix has a very simple form (\(0\) and \$1\$s or if we're just counting number of multiplication rather the actual bits like before).
\subsubsection{Divide and Conquer}
\label{sec:orgbcdcb10}
Split \(A,B\) into four \(\frac{n}{2}\times \frac{n}{2}\) blocks
$$ A = \begin{bmatrix*}[r] A_{00} & A_{01} \\ A_{10} & A_{11} \end{bmatrix*}, B = \begin{bmatrix*}[r] B_{00} & B_{01} \\ & B_{10} B_11 \end{bmatrix*}$$
where \(A_{00},A_{01}, A_{10}, A_{11}, B_{00}, \ldots, B_{11}\) are \(\frac{n}{2} \times \frac{n}{2}\) matrices.
\begin{itemize}
\item Example: \begin{center}
\includegraphics[width=.9\linewidth]{./Images/i80.png}
\end{center}
\end{itemize}
$$AB = \begin{bmatrix*}[r] \underbrace{A_{00}B_{00}+A_{01}B_{10}}_{\frac{n}{2}\times \frac{n}{2}} & A_{00}B_{01}+A_{01}+B_{11}
\\ A_{10}B_{00}+A_{11}B_{10} & A_{10}B_{01}+A_{11}B_{11}
\end{bmatrix*}$$
$$T(n)=8T(n/3) (multiplications)+O(n^2) (additions)$$
Recursively compute:

\begin{equation*}
A_{00}B_{00},A_{01}B_{10},A_{00}B_{01}, \ldots
\end{equation*}

Using Master Theorem:
\begin{align*}
\log_2(8)>2 \implies \Theta(n^3)
\\
\begin{bmatrix*}[r] 
C_{00} & C_{01} \\ C_{10} & C_{11} 
\end{bmatrix*} 
= 
\begin{bmatrix*}[r]
A_{00} & A_{01} \\ A_{10} & A_{11} 
\end{bmatrix*} 
\times 
\begin{bmatrix*}
[r]B_{00} & B_{01} \\ B_{10} & B_{11}
\end{bmatrix*}
\end{align*}

\begin{align*}
P_1 & = A_{00}\times (B_{01}-B_{11})
\\ P_2 & = (A_{00}+A_{01})B_{11}
\\ P_3 & = (A_{10}+A_{11})B_{00}
\\ P_4 & = A_{11}(B_{10}-B_{11})
\\P_5 & = (A_{00}+A_{11})(B_{00}+B_{11})
\\P_6 & = (A_{01}-A_{11})(B_{10}+B_{11})
\\ P_7 & = (A_{00}-A_{10})(B_{00}+B_{01})
\\ \hline
\\ C_{00} & = P_5+P_4-P_2-P_6
\\ C_{01} & = P_1+P_2
\\ C_{10} & = P_3+P_4
\\ C_{11} & = P_5+P_1-P_3-P_2
\end{align*}
Strassen Alg: 1971
$$ T(n/2)+O(n^2) (additions)$$
\begin{itemize}
\item Compute: \(P_1,\ldots,P_7\) recursively (all size \(\frac{n}{2}\times \frac{n}{2}\))
\item Compute: \(C_{00}, C_{01},C_{10},C_{11}\) using them
\end{itemize}
$$T(n)=7 \times T(n/2)+O(n^2)(additions/subtractions)$$
$$\log_2 7 > 2 \implies \Theta (n^{\log_2 7})=\Theta(n^{2.81\ldots})$$
Matrix computations are everywhere, graphics do matrix computations all the time. 
\begin{itemize}
\item Can this be improved? Has been improved to something like \(\Theta(n^{2.37\ldots})\)
\end{itemize}
Open Problem: Can we do matrix multiplication in \(O(n^{2+O(1)})\)?
\begin{itemize}
\item Say \(O(n^2(\log n)^{1000})\)
\item We don't know
\end{itemize}
\section{Lecture 19 \textit{<2017-11-14 Tue>}}
\label{sec:org6a25453}
\subsection{Dynamic Programming}
\label{sec:org34b7a9f}
Each instance of the problem is solved by "looking up" the solution to smaller instances of the problem. Hence often we fill up a table with solutions to instances of the problem, starting from smaller values of a parameter, and each one looks up the earlier values in the table to compute the current entry.

\noindent\rule{\textwidth}{0.5pt}
Name "Dynamic" was suggested by Bellman in the 50's. Was very theoretical but not much money was given to theoretical things during the time. Wanted to make the name nicer to get more funding, even though this name doesn't fit that much.
\subsubsection{Weighted interval scheduling}
\label{sec:org9c16c00}
\uline{Recall}: Weighted interval scheduling
Input: Jobs \((s_i, f_i, v_i)\), where \(s\) is starting time, \(f\) is finishing time and \(v\) is the value, \(v_i\geq 0\)

\uline{Goal}: Find the maximum value, non-overlapping set of jobs.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i81.png}
\end{center}

\noindent\rule{\textwidth}{0.5pt}
Remark: Setting all \(v_i\) to \(1\) recovers the simple interval scheduling problem (maximizing \# of jobs)

\uline{Recall}: In the original interval scheduling problem, we showed that the greedy algorithm that sorts the jobs \(f_1 \leq f_2 \leq \ldots \leq f_n\) and picks each job iff it is consistent with the earlier chosen jobs. In the above example, we would choose \(1,2,1\) on the top (ignoring values) and only getting \(4\) instead of \(6\).
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i82.png}
\end{center}
\begin{itemize}
\item Come up with any greedy algorithm, won't work. If you pick biggest value first, the above example won't work. If you look at ratios between time and values it won't work, etc.
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Let us still sort them by their finishing time
$$ f_1 \leq f_2 \leq \ldots \leq f_n$$
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i83.png}
\end{center}
\begin{itemize}
\item Consider the problem until time \(f_j\) -> Two cases:
\begin{itemize}
\item Reject job \(j\) ->
\begin{itemize}
\item We are optimizing value until job \(j-1\)
\end{itemize}
\item Accept job \(j\) -> gain \(v_i +\) we are optimizing value until the \uline{last job where finishing time is before \(s_j\)} (call this \(P(j)\))
\end{itemize}
\end{itemize}
$$ P(j) = \max \{i | f_i \leq s_j\}$$

\noindent\rule{\textwidth}{0.5pt}
Def: \(Opt[j]=\) maximum profit that we can make from jobs \(1, \ldots, j\)
\begin{itemize}
\item We just showed \(Opt[j] = \max \{Opt[j-1], v_j + Opt[P(j)]\}\) with base case \(Opt[0]=0\). Not that \(Opt[P(j)]\) considers jobs \(1,\ldots,P(j)\)
\end{itemize}
\begin{enumerate}
\item Alg
\label{sec:org985215b}
\begin{itemize}
\item Sort the jobs so that
\end{itemize}
$$f_1 \leq f_2 \leq \ldots \leq f_n, O(n \log n)$$
\begin{itemize}
\item Compute the values \(P(j)\) for \(j=1,\ldots,n\). Can be done easily in \(O(n \log n)\) via binary search, can even be possible to do in \(O(n)\) but we already have \(O(n \log n)\) from sorting above anyway.
\item \(Opt[0] = 0\)
\end{itemize}
\begin{algorithmic}
\For {$j=1,\ldots,n$}
    \State $Opt[j] = \max\{Opt[j-1],v_j+Opt[P(j)]\}$
    \State // Note that $Opt[j-1]$ and $Opt[P(j)]$ are already computed and in an array.
\EndFor
\State Return $Opt[n]$
\end{algorithmic}
Total -> \(O(n \log n)\)

What if we implement this as a recursive program?
\begin{itemize}
\item Sort \(f_1 \leq \ldots \leq f_n\)
\item Compute \(P(j)\) for \(j=1,\ldots,n\)
\end{itemize}
\begin{algorithmic}
\State ComputeOpt(j)
\If {$j==0$}
    \State return $0$
\Else
    \State return $\max(ComputeOpt(j-1), v_j+ComputeOpt(P(j)))$
\EndIf
\end{algorithmic}
Running time? You have recursion and a binary structure that will just go down. This can get exponential.
\begin{itemize}
\item Example: \begin{center}
\includegraphics[width=.9\linewidth]{./Images/i84.png}
\end{center}
\end{itemize}
Running time: \(O(2^n)\). But why? This is similar to the way we did it dynamically. We recompute opt values many times, and this leads to exponential time.
\begin{itemize}
\item Another example: \begin{center}
\includegraphics[width=.9\linewidth]{./Images/i85.png}
\end{center}
\end{itemize}
Storing the values in an array and not recomputing them is dynamic programming. Really just recursion except we save the values.
\item Memorization
\label{sec:orgb21dc56}
\begin{algorithmic}
\State Sort $f_1 \leq \ldots \leq f_n$
\State Compute $P(j)$, $j=1,\ldots,n$
\For {$j=1,\ldots,n$}
     \State $Opt[j]=$ empty
\EndFor
\State $Opt[0]=0$
\end{algorithmic}
\begin{algorithmic}
\State ComputeOpt(j)
    \If {$Opt[j] = empty$}
    	\State $Opt[j] = \max \{ComputeOpt(j-1), v_j+ComputeOpt(P(j))\}$
    \Else
	\State return $Opt[j]$
\EndIf	
\end{algorithmic}
Now for every \(j\) we make two \uline{recursive calls} at most once. -> \(O(2\times n) = O(n)\) for recursive step -> \(O(n \log n)\) total time.

\noindent\rule{\textwidth}{0.5pt}
This memorization technique is useful when computing most of the entries in the table are not necessary.
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i86.png}
\end{center}
\end{enumerate}
\subsubsection{Knapsack Problem}
\label{sec:orga2d6125}
(Will be also used in COMP360 with Hatami, important)
\begin{itemize}
\item Given \(n\) objects and a "knapsack"
\item Item \(i\) has weight \(w_i>0\) (integers)
\begin{itemize}
\item value \(v_i>0\)
\end{itemize}
\item Knapsack has capacity \(W\) (in weight)
\end{itemize}
\uline{Goal}: Fill the knapsack so as to maximize the value without exceeding its capacity.
\begin{itemize}
\item Ex:
\end{itemize}
\begin{center}
\begin{tabular}{llll}
 & \(w_i\) & \(v_i\) & ratio\\
\hline
\#1 & \(1\) & \(1\) & \(1\)\\
\#2 & \(2\) & \(6\) & \(3\)\\
\#3 & \(5\) & \(18\) & \(3 \frac{3}{5}\)\\
\#4 & \(6\) & \(22\) & \(3 \frac{2}{3}\)\\
\#5 & \(7\) & \(28\) & \(4\)\\
\end{tabular}
\end{center}
Optimal total value: \(40\)

\noindent\rule{\textwidth}{0.5pt}
Greedy Alg: Compute value/weight ratio and start from the items with higher ratio -> \#5, \#2, \#1 -> \(value = 28+6+1=35\)

Optimal \#3, \#4 -> \(40\)

This is an NP complete problem. Can't solve it greedily. Let's try and solve it.

\uline{Attempt \#2}: 
Define \(Opt[i]=\max\) value if we only consider items \(1,\ldots,j\)

Two cases: 
\begin{itemize}
\item Case 1: Not include item \(j\)
\begin{itemize}
\item \(Opt[j-1]\) can be achieved
\end{itemize}
\item Case 2: Item \(j\) in the knapsack. Value \(v_j\) included.
\begin{itemize}
\item \(v_j+Opt[j-1]\)?
\begin{itemize}
\item No because this will use \(w_j\) of the capacity!
\end{itemize}
\item It is \(v_j+Opt(j-1, \text{ new cap}=W-w_j)\)
\end{itemize}
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
\uline{Solution}:
\(Opt[j,u]=\max\) value if we consider items \(1,\ldots,j\) with capacity \(u\), where \(u=0, \ldots, W\)

\noindent\rule{\textwidth}{0.5pt}
\begin{algorithmic}
\For{$u=0,\ldots,W$}
	\State $Opt[0,u]=0$
\EndFor
\For{$j=1, \ldots, n$}
	   \State $Opt[j,u] = \max \{opt[j-1,u], v_j+opt[j-1,u-w_j]\}$
	   \State // Only if $w_j \leq u$
\EndFor
\end{algorithmic}
Running time: \(O(nW)\). This isn't considered polynomial. \(W\) is just a number here and may be a number to a big exponent.
\section{Lecture 20 \textit{<2017-11-16 Thu>}}
\label{sec:org1d1e695}
\subsection{Recall}
\label{sec:orgcb743cf}
Knapsack \(\Theta(nW)\)
\begin{itemize}
\item Input: Weights \(w_1,\ldots,w_n\)
\begin{itemize}
\item Values \(v_1,\ldots,v_n\)
\item Capacity \(W\)
\end{itemize}
\item \(Opt[i,u]=\) optimal value items \(1,\ldots,i\) capacity \(u\)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
Weren't able to solve this greedily, but we did come up with a recursive algorithm that could be implemented through dynamic programming.
\begin{equation*}
Opt[i,u] = \max 
\begin{cases}
opt[i-1, u]
\\ & \text{if } w_i\leq u \ v_i + opt[i-1, u-w_i]
\end{cases}
\end{equation*}
\(Opt[n,W]=\) Final answer

This is not a polynomial time algorithm. Why?
\begin{itemize}
\item Recall: An algorithm is polynomial time if it's running time is \(O(N^c)\) where \(c\) is a constant and \(N\) is the size of the input (in bits)
\end{itemize}

\noindent\rule{\textwidth}{0.5pt}
What is \(N\) here?
\begin{itemize}
\item input \(w_1, \ldots, w_n\)
\begin{itemize}
\item \(v_1, \ldots, v_n\)
\item \(W\)
\end{itemize}
\end{itemize}
How many bits do we need for this? N?
$$ N = O(\log w_1 + \log w_2 + \ldots + \log w_n + \log v_1 + \log v_2 + \ldots + \log v_n + \log W) $$
Where here logs are base 2 (for number of bits to store the number in binary)

Ex: \(w_1=2, w_2=4, v_1=3, v_2=1, w=2^{10}\)
$$10, 100, 11, 1, 1\underbrace{00 \ldots 0}_10$$

\noindent\rule{\textwidth}{0.5pt}
Running Time \(\Theta(nW)\)
\begin{itemize}
\item Let us show that this is not polynomial time:
\item \(W = 2^n\) and \(w_1 = \ldots = w_n = v_1 = \ldots = v_n = 2\)
\item \(N=O(\log w_1 + \ldots + \log w_v + \log v_1 + \ldots + \log v_n + \log W) = O(n)\)
\end{itemize}
Running Time: \(\Theta(nW) = \Theta(n2^n)\)
\subsection{Another Example}
\label{sec:orgc530851}
Input \(n\)
\begin{itemize}
\item Goal: Is \(n\) a prime?
\end{itemize}
\begin{algorithmic}
\For {$i=2, \ldots, n-1$}
     \If{$(n \mod i) = 0$}
     	     \State return ``Not prime"
     \EndIf 
\EndFor
\State return Prime
\end{algorithmic}

\noindent\rule{\textwidth}{0.5pt}
Running time \(\Omega(n)\)
\begin{itemize}
\item Input size: \(N=\log_2 n\)
\item Running time: \(\Omega(n)=\Omega(2^N)\)
\end{itemize}
\subsection{RNA secondary structures}
\label{sec:orgdd12d0c}
RNA: String \(B=b_1,\ldots,b_n\) over alphabet \(\{A,C,G,U\}\).
\begin{itemize}
\item Ex: \(GUCGAUUAGC \ldots GA\)
\item Secondary Structure: RNA's tend to loop back and make pairs with itself
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i87.png}
\end{center}
Secondary Structure: Is a set of pairs \(S=\{(b_i,b_j), \ldots\}\) that satisfies three properties

[Watson-Crick (Property)]: \(S\) is a matching and each pair is a Watson-Crick complement: \(A-U, U-A, C-G, G-C\)

[No sharpturns]: If \((b_i,b_j), i<j\) is a pair then \(i<j-4\) (at least distance \(4\) between a pair)

[No crossings]: We can't have \((b_i,b_j),(b_k,b_l)\) with \$i<k<j<l
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i88.png}
\end{center}
RNA molecules tend to form a structure with maximum number of pairings
\begin{itemize}
\item Example: \(AUGUGGCCAU\)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i89.png}
\end{center}
\begin{itemize}
\item Bad pairing:
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Images/i90.png}
\end{center}

Problem: Given an RNA sequence find the maximum number of pairings.

\noindent\rule{\textwidth}{0.5pt}
Attempt \#1: Let \(Opt[j]\) to be the optimal number of pairs until point \(j\) : \(b_1,\ldots,b_j\)
\begin{itemize}
\item Case \(1\): \(b_j\) is not in any pair
\begin{itemize}
\item \(Opt[j] = Opt[j-1]\)
\end{itemize}
\item Case \(2\): \(b_j\) is paired with \(b_t\)
\end{itemize}
$$Opt[i,j] = 1 + Opt[1_t-1]+Opt[t+1,j-1]$$
$$Opt[i,j] = \max \{Opt[i,t-1]+Opt[t+1,j-1]\}$$
\begin{itemize}
\item Max is over all \(t\) such that \$(b\(_{\text{t,v}}\)\(_{\text{j}}\)) is a \uline{legal} pair.
\begin{itemize}
\item \(i \leq t < j-4\)
\item \((b_t,b_j) \in \{AU, UA, CG, GC\}\)
\end{itemize}
\end{itemize}
\(Opt[i,j] = 0\) if \(i \geq j-4\)
Can be implemented using memorization (recursion) or can be implemented with loops

\noindent\rule{\textwidth}{0.5pt}
The second way to implement this:
\begin{algorithmic}
\If {$i \geq j$}
    \State $Opt[i,j]=0$
\EndIf
\For {$k=5,\ldots,n-1$}
     \For{$i=1, \ldots, n-k$}
     		\State $j=i+k$
		\State Compute $Opt[i,j]$ using the formula we obtained earlier.
     \EndFor
\EndFor
\end{algorithmic}

\noindent\rule{\textwidth}{0.5pt}
Running time \(O(n^3)\) because Compute \(Opt[i,j]\) takes \(O(n)\)
\end{document}