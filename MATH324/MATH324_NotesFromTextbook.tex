\documentclass[12 pt]{article}
\usepackage{hyperref, fancyhdr, setspace, enumerate, amsmath,
  lastpage, amssymb, tabularx}
\usepackage[margin=1 in]{geometry}
\allowdisplaybreaks
%\usepackage[dvipsnames]{xcolor}   %May be necessary if you want to color links
\hypersetup{
	%colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=black,  %choose some color if you want links to stand out
}
\usepackage{graphicx}
\graphicspath{{Images/}}

\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\theoremstyle{definition}
\newtheorem{defn}{Definition}

% Independent symbol
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\author{Julian Lore}
\date{Last updated: \today}
\title{MATH 324: Statistics}
\pagestyle{fancy}
\lhead{MATH 324}
\chead{\leftmark}
\rhead{Julian Lore}
\cfoot{Page \thepage \ of \pageref{LastPage}}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
\begin{document}
	\onehalfspacing
	\maketitle
	Rough notes from Wackerly's Mathematical Statistics with
        Applications (7\textsuperscript{th} edition).
	\tableofcontents
        \section*{Information from Previous Chapters}
        Some formulas from probability:
        \begin{align*}
          Var(X) & = E[(X-\mu)^2] = E[X^2] - (E[X])^2
        \end{align*}
        Distributions:
        % Add the others later
        \begin{center}
          \begin{tabularx}{\textwidth}{c c c c c}
            \hline Distribution & Probability Function & Mean &
            Variance & MGF
            \\ \hline Binomial & $p(y)=\binom{n}{y}p^y(1-p)^{n-y}$ &
            $np$ & $np(1-p)$ & $[pe^t+(1-p)]^n$
            \\ Geometric & $p(y) = p(1-p)^{y-1}$ & $\frac{1}{p}$ &
            $\frac{1-p}{p^2}$ & $\frac{pe^t}{1-(1-p)e^t}$
            \\ Hypergeometric & $p(y) = \frac{\binom{r}{y}
              \binom{N-r}{n-y}}{\binom{N}{n}}$ & $\frac{nr}{N}$ & $n
            \left(\frac{r}{N}\right) \left(\frac{N-r}{N}\right)
            \left(\frac{N-n}{N-1}\right) $ & No closed form
            \\ Poisson & $p(y) = \frac{\lambda^y e^{-\lambda}}{y!}$ &
            $\lambda$ & $\lambda$ & $e^{\lambda(e^t-1)}$
            \\ Negative binomial & $p(y) = \binom{y-1}{r-1} p^r
            (1-p)^{y-r}$ & $\frac{r}{p}$ & $\frac{r(1-p)}{p^2}$ &
            $\left(\frac{pe^t}{1-(1-p)e^t}\right)^r$
            \\ \hline Uniform & $f(y) = \frac{1}{\theta_2 - \theta_1}$
            & $\frac{\theta_1+\theta_2}{2}$ &
            $\frac{(\theta_2-\theta_1)^2}{12}$ & $\frac{e^{t
                \theta_2}-e^{t \theta_1}}{t(\theta_2 - \theta_1)}$
            \\ Normal & $f(y) = \frac{1}{\sigma \sqrt{2\pi}}e^{-
              \left(\frac{1}{2\sigma^2}\right)(y-\mu)^2}$ & $\mu$ &
            $\sigma^2$ & $e^{\mu t + \frac{t^2 \sigma^2}{2}}$
            \\ Exponential & $f(y) =
            \frac{1}{\beta}e^{-\frac{y}{\beta}}$ & $\beta$ & $\beta^2$
            & $(1-\beta t)^{-1}$
            \\ Gamma & $f(y)=
            \left(\frac{1}{\Gamma(\alpha)\beta^\alpha}\right)y^{\alpha
              - 1}e^{-\frac{y}{\beta}}$ & $\alpha \beta$ & $\alpha
            \beta^2$ & $(1-\beta t)^{-\alpha}$
            \\ Chi-square & $f(y) =
            \frac{y^{\frac{\nu}{2}-1}e^{-\frac{y}{2}}}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}$
            & $\nu$ & $2 \nu$ & $(1-2t)^{-\frac{\nu}{2}}$
            \\ Beta & $f(y)= \left(\frac{\Gamma(\alpha +
                \beta)}{\Gamma(\alpha)\Gamma(\beta)}\right)y^{\alpha-1}(1-y)^{\beta
              - 1}$ & $\frac{\alpha}{\alpha + \beta}$ & $\frac{\alpha
              \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$ & No closed form
          \end{tabularx}
        \end{center}
        \subsection*{6.7 Order Statistics}
        $Y_i$, with $i=1,\ldots,n$ independent continuous random
        variables. Denote ordered random variables by: $Y_{(1)} \leq
        Y_{(2)} \leq \ldots \leq Y_{(n)}$. We can then define:
        \begin{align*}
          Y_{(1)} & = \min{(Y_1, Y_2, \ldots, Y_n)}
          \\ Y_{(n)} & = \max{(Y_1, Y_2, \ldots, Y_n)}
        \end{align*}
        In order to find the pdf of $Y_{(1)}$ and $Y_{(n)}$, we use
        the method of distribution functions.
        \begin{align*}
          F_{Y_{(n)}}(y) & = P(Y_{(n)} \leq y) = P(Y_1\leq y, Y_2 \leq y, \ldots, Y_n \leq y)
          \\ &\stackrel{\text{ind}}{=}P(Y_1 \leq y)P(Y_2 \leq y)\ldots p(Y_n \leq y) = [F(y)]^n
               \intertext{Derive for the density:}
               g_{(n)}(y) & = n[F(y)]^{n-1}f(y)
        \end{align*}
        \noindent \rule{\textwidth}{0.5pt}
        \begin{align*}
          \\ F_{Y_{(1)}}(y) & = P(Y_{(1)} \leq 1) = 1 - P(Y_{(1)}>y) = 1 - 1 - P(Y_1>y, Y_2 > y, \ldots Y_n > y)
          \\ &\stackrel{\text{ind}}{=} 1-[P(Y_1 > y)P(Y_2>y)\ldots P(Y_n)>y] = 1-[1-F(y)]^n
               \intertext{Derive for the density:}
               g_{(1)}(y) & = n[1-F(y)]^{n-1}f(y)
        \end{align*}
        \section*{Other Useful Information}
        \begin{align*}
          E(\overline{Y}) & = E(Y)
        \end{align*}
        \setcounter{section}{7}
        \section{Estimation}
        \subsection{Introduction}
        Point of statistics is to use sample information to infer data
        about the population. Populations are characterized by numbers
        (\textit{parameters}) and we often want to estimate the value
        of parameter(s). Parameters include the proportion $p$,
        population mean $\mu$, variance $\sigma^2$ and standard
        deviation $\sigma$.
        \begin{defn}
          The parameter of interest in an experiment is called the
          \textit{target parameter}.
        \end{defn}
        \begin{defn}
          A \textit{point estimate} is a type of estimate where we use
          a single value/point to estimate a parameter. If we estimate
          a parameter by saying that it might fall between two
          numbers, this is an \textit{interval estimate}. We can use
          information from the sample to calculate these estimates,
          which are done using an estimator.
        \end{defn}
        \begin{defn}
          An \textit{estimator} is a rule, often expressed as a
          formula, that tells how to calculate the value of an
          estimate based on the measurements contained in a sample.
        \end{defn}
        \begin{defn}
          \textit{Sample mean:}
          $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$$ This is an
          example point estimator of $\mu$.
        \end{defn}
        There can be different estimators for the same population
        parameter. Some estimators are considered good and others are bad.
        \subsection{The Bias and Mean Square Error of Point
          Estimators}
        We cannot measure how good a point estimation procedure is
        with a single estimate, we need to observe the procedure many
        times. We create a frequency distribution to measure the
        goodness of a point estimator.
        \paragraph{Point Estimators} For a population parameter
        $\theta$, the estimator of $\theta$ is called $\hat{\theta}$.
        \begin{defn}
        Ideally, we'd want $E(\hat{\theta}) = \theta$. Point
        estimators that satisfy this are called
        \textit{unbiased}. Otherwise, they are called \textit{biased},
        where the \textit{bias} is given by
        $B(\hat{\theta})=E(\hat{\theta})-\theta$
      \end{defn}
      In addition, we'd also like the estimator $V(\hat{\theta})$ to
      be as small as possible, since a smaller variance guarantees a
      higher fraction of estimators to be ``close'' to $\theta$. If
      two estimators are unbiased and everything else is equal other
      than variance, we prefer the one with smaller variance.
      \begin{defn}
        Another way to characterize goodness of a point estimator is
        via its \textit{mean square error},
        $$MSE(\hat{\theta}) = E[(\hat{\theta}-\theta)^2]$$
        Which is the average of the square of the distance between the
        estimator and its target parameter. It can be shown that:
        $$MSE(\hat{\theta})=V(\hat{\theta})+[B(\hat{\theta})]^2$$
      \end{defn}
      \subsection{Some Common Unbiased Point Estimators}
      \begin{defn}
        $\sigma_{\hat{\theta}}^2$ denotes the variance of the sampling
        distribution of the estimator $\hat{\theta}$. The standard
        deviation $\sigma_{\hat{\theta}} =
        \sqrt{\sigma_{\hat{\theta}}^2}$ is called the \textit{standard
        error} of the estimator $\hat{\theta}$.
    \end{defn}
    \paragraph{Common Point Estimators} If random samples are
    independent we have:
    \\
    \begin{tabular}{|c c c c c|}
      \hline Target Parameter $\theta$ & Sample Size(s) & Point
      Estimator $\hat{\theta}$ & $E(\hat{\theta})$ & Standard Error
      $\sigma_{\hat{\theta}}$
      \\ \hline $\mu$ & $n$ & $\overline{Y}$ & $\mu$ &
      $\frac{\sigma}{\sqrt{n}}$
      \\ $p$ & $n$ & $\hat{p} = \frac{Y}{n}$ & $p$ &
      $\sqrt{\frac{pq}{n}}$
      \\ $\mu_1 - \mu_2$ & $n_1, n_2$ & $\overline{Y}_1 -
      \overline{Y}_2$ & $\mu_1 - \mu_2$ &
      $\sqrt{\frac{\sigma_1^2}{n_1}+ \frac{\sigma_2^2}{n_2}}$
      \\ $p_1 - p_2$ & $n_1,n_2$ & $\hat{p}_1 - \hat{p}_2$ & $p_1 -
      p_2$ & $\sqrt{\frac{p_1q_1}{n_1}+\frac{p_2q_2}{n_2}}$
      \\ \hline
    \end{tabular}\\
    Note, these are all \textbf{unbiased} estimators. Note that the expected
    values and standard errors for $\overline{Y}$ and $\overline{Y}_1
    - \overline{Y}_2$ are valid for any distribution of the population
    and all four estimators are approximately normal for large $n$
    (see CLT).
    \begin{defn}
      The sample variances,
      $$S^2 = \dfrac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}$$
      $$S'^2 = \dfrac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}$$
      are unbiased ($S^2$) and biased ($S'^2$) estimators for $\sigma^2$.
    \end{defn}
    \subsection{Evaluating the Goodness of a Point Estimator}
    \begin{defn}
      The \textit{error of estimation} $\varepsilon$ is the distance
      between an estimator and its target parameter:
      $$\varepsilon = |\hat{\theta} - \theta|$$
    \end{defn}
    Note that $\hat{\theta}$ is a random variable, so the error of
    estimation is a random quantity and thus we can make probability
    statements about it (rather than saying exactly how big it is for
    some estimate).

    The probability $P(\varepsilon < b)$ represents the fraction of
    times, in repeated sampling that $\hat{\theta}$ falls within $b$
    units of $\theta$.

    Sometimes we want to find a value of $b$ such that $P(\varepsilon
    < b) =$ some value, say $0.90$.

    If $\hat{\theta}$ is unbiased we can find a bound on $\varepsilon$
    by expressing $b$ as a multiple of the standard error. Take for
    example, $b=k \sigma_{\hat{\theta}}$, where $k \geq 1$, then by
    Tchebysheff's we know that $P(\varepsilon < k
    \sigma_{\hat{\theta}}) \geq 1 -
    \frac{1}{k^2}$. $b=2\sigma_{\hat{\theta}}$ is a good approximate
    bound on $\varepsilon$ in practice. By Tchebysheff we know that
    $P(\varepsilon < 2\sigma_{\hat{\theta}}) \geq .75$.
    \subsection{Confidence Intervals}
    \begin{defn}
      An \textit{interval estimator} is a rule specifying the method
      for using sample measurements to calculate two numbers
      (endpoints) of the interval. We want the interval to contain
      $\theta$ and for it to be \textbf{narrow}. One or both of the
      endpoints are functions of the sample measurements and thus vary
      randomly from sample to sample, i.e. cannot be sure that
      $\theta$ falls in the interval, so we want an interval estimator
      with a high probability of containing $\theta$.
    \end{defn}
    \begin{defn}
      Interval estimators are commonly called \textit{confidence intervals}.
    \end{defn}
    \begin{defn}
      Upper/lower endpoints are called \textit{upper} and
      \textit{lower confidence limits}, sometimes denoted
      $\hat{\theta}_L$ and $\hat{\theta}_U$.
    \end{defn}
    \begin{defn}
      Probability that a random confidence interval will enclose
      $\theta$ is called the \textit{confidence coefficient}, denoted
      by $(1 - \alpha)$. In
      practice, this coefficient describes the fraction of time that
      random/repeated sampling intervals will contain $\theta$.

      We have: $$P(\hat{\theta}_L \leq \theta \leq \hat{\theta}_U) = 1 - \alpha$$
    \end{defn}
    \begin{defn}
      $[\hat{\theta}_L, \hat{\theta}_U]$ is called a \textit{two-sided
      confidence interval}. A \textit{one-sided confidence interval}
    satisfies $[\hat{\theta}_L,\infty)$, an upper one-sided confidence
    interval is $(-\infty, \hat{\theta}_U]$.
  \end{defn}
  \begin{defn}
    The \textit{pivotal method} is very useful for finding confidence
    intervals. We find a pivotal quantity that satisfies:
    \begin{enumerate}
    \item Being a function of sample measurements and $\theta$, where
      $\theta$ is the only unknown quantity.
    \item Probability distribution does not depend on $\theta$.
    \end{enumerate}
    We can then say:
    \begin{align*}
      P(a \leq Y \leq b) = p &\implies P(ca \leq cY \leq cb) = p
      \\ & \implies P(a+d \leq Y+d \leq b+d) = p
    \end{align*}
    i.e. A change of scale or translation of $Y$ does not change the
    probability. So knowing the probability distribution of a pivotal
    quantity, we can use these operations and get the interval.
  \end{defn}
  \subsection{Large-Sample Confidence Intervals}
  If $\theta = \mu, p, \mu_1 - \mu_2$ or $p_1 - p_2$ then for large
  samples $$Z =
  \frac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\stackrel{approx}{\sim}Normal(0,1)$$
  Also, $Z$ is (approximately) a pivotal quantity.

  We also have:

  \begin{align*}
    100(1-\alpha)\% \text{ lower bound for }\theta = \hat{\theta}-z_{\alpha}\sigma_{\hat{\theta}}
    \\100(1-\alpha)\% \text{ upper bound for }\theta = \hat{\theta}+z_{\alpha}\sigma_{\hat{\theta}}
    \\ \implies 100(1-2\alpha)\%\theta = \hat{\theta}\pm z_{\alpha}\sigma_{\hat{\theta}}
  \end{align*}
\end{document}