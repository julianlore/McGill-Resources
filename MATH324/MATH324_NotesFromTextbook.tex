\documentclass[12 pt]{article}
\usepackage{hyperref, fancyhdr, setspace, enumerate, amsmath,
  lastpage, amssymb, tabularx}
\usepackage[margin=1 in]{geometry}
\allowdisplaybreaks
%\usepackage[dvipsnames]{xcolor}   %May be necessary if you want to color links
\hypersetup{
	%colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=black,  %choose some color if you want links to stand out
}
\usepackage{graphicx}
\graphicspath{{Images/}}

\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\theoremstyle{definition}
\newtheorem{defn}{Definition}

% Independent symbol
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\author{Julian Lore}
\date{Last updated: \today}
\title{MATH 324: Statistics}
\pagestyle{fancy}
\lhead{MATH 324}
\chead{\leftmark}
\rhead{Julian Lore}
\cfoot{Page \thepage \ of \pageref{LastPage}}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
\begin{document}
	\onehalfspacing
	\maketitle
	Rough notes from Wackerly's Mathematical Statistics with
        Applications (7\textsuperscript{th} edition).
	\tableofcontents
        \section*{Information from Previous Chapters}
        Some formulas from probability:
        \begin{align*}
          Var(X) & = E[(X-\mu)^2] = E[X^2] - (E[X])^2
        \end{align*}
        Distributions:
        % Add the others later
        \begin{center}
          \begin{tabularx}{\textwidth}{c c c c c}
            \hline Distribution & Probability Function & Mean &
            Variance & MGF
            \\ \hline Binomial & $p(y)=\binom{n}{y}p^y(1-p)^{n-y}$ &
            $np$ & $np(1-p)$ & $[pe^t+(1-p)]^n$
            \\ Geometric & $p(y) = p(1-p)^{y-1}$ & $\frac{1}{p}$ &
            $\frac{1-p}{p^2}$ & $\frac{pe^t}{1-(1-p)e^t}$
            \\ Hypergeometric & $p(y) = \frac{\binom{r}{y}
              \binom{N-r}{n-y}}{\binom{N}{n}}$ & $\frac{nr}{N}$ & $n
            \left(\frac{r}{N}\right) \left(\frac{N-r}{N}\right)
            \left(\frac{N-n}{N-1}\right) $ & No closed form
            \\ Poisson & $p(y) = \frac{\lambda^y e^{-\lambda}}{y!}$ &
            $\lambda$ & $\lambda$ & $e^{\lambda(e^t-1)}$
            \\ Negative binomial & $p(y) = \binom{y-1}{r-1} p^r
            (1-p)^{y-r}$ & $\frac{r}{p}$ & $\frac{r(1-p)}{p^2}$ &
            $\left(\frac{pe^t}{1-(1-p)e^t}\right)^r$
            \\ \hline Uniform & $f(y) = \frac{1}{\theta_2 - \theta_1}$
            & $\frac{\theta_1+\theta_2}{2}$ &
            $\frac{(\theta_2-\theta_1)^2}{12}$ & $\frac{e^{t
                \theta_2}-e^{t \theta_1}}{t(\theta_2 - \theta_1)}$
            \\ Normal & $f(y) = \frac{1}{\sigma \sqrt{2\pi}}e^{-
              \left(\frac{1}{2\sigma^2}\right)(y-\mu)^2}$ & $\mu$ &
            $\sigma^2$ & $e^{\mu t + \frac{t^2 \sigma^2}{2}}$
            \\ Exponential & $f(y) =
            \frac{1}{\beta}e^{-\frac{y}{\beta}}$ & $\beta$ & $\beta^2$
            & $(1-\beta t)^{-1}$
            \\ Gamma & $f(y)=
            \left(\frac{1}{\Gamma(\alpha)\beta^\alpha}\right)y^{\alpha
              - 1}e^{-\frac{y}{\beta}}$ & $\alpha \beta$ & $\alpha
            \beta^2$ & $(1-\beta t)^{-\alpha}$
            \\ Chi-square & $f(y) =
            \frac{y^{\frac{\nu}{2}-1}e^{-\frac{y}{2}}}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}$
            & $\nu$ & $2 \nu$ & $(1-2t)^{-\frac{\nu}{2}}$
            \\ Beta & $f(y)= \left(\frac{\Gamma(\alpha +
                \beta)}{\Gamma(\alpha)\Gamma(\beta)}\right)y^{\alpha-1}(1-y)^{\beta
              - 1}$ & $\frac{\alpha}{\alpha + \beta}$ & $\frac{\alpha
              \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$ & No closed form
          \end{tabularx}
        \end{center}
        \subsection*{6.7 Order Statistics}
        $Y_i$, with $i=1,\ldots,n$ independent continuous random
        variables. Denote ordered random variables by: $Y_{(1)} \leq
        Y_{(2)} \leq \ldots \leq Y_{(n)}$. We can then define:
        \begin{align*}
          Y_{(1)} & = \min{(Y_1, Y_2, \ldots, Y_n)}
          \\ Y_{(n)} & = \max{(Y_1, Y_2, \ldots, Y_n)}
        \end{align*}
        In order to find the pdf of $Y_{(1)}$ and $Y_{(n)}$, we use
        the method of distribution functions.
        \begin{align*}
          F_{Y_{(n)}}(y) & = P(Y_{(n)} \leq y) = P(Y_1\leq y, Y_2 \leq y, \ldots, Y_n \leq y)
          \\ &\stackrel{\text{ind}}{=}P(Y_1 \leq y)P(Y_2 \leq y)\ldots p(Y_n \leq y) = [F(y)]^n
               \intertext{Derive for the density:}
               g_{(n)}(y) & = n[F(y)]^{n-1}f(y)
        \end{align*}
        \noindent \rule{\textwidth}{0.5pt}
        \begin{align*}
          \\ F_{Y_{(1)}}(y) & = P(Y_{(1)} \leq 1) = 1 - P(Y_{(1)}>y) = 1 - 1 - P(Y_1>y, Y_2 > y, \ldots Y_n > y)
          \\ &\stackrel{\text{ind}}{=} 1-[P(Y_1 > y)P(Y_2>y)\ldots P(Y_n)>y] = 1-[1-F(y)]^n
               \intertext{Derive for the density:}
               g_{(1)}(y) & = n[1-F(y)]^{n-1}f(y)
        \end{align*}
        \section*{Other Useful Information}
        \begin{align*}
          E(\overline{Y}) & = E(Y)
        \end{align*}
        \setcounter{section}{7}
        \section{Estimation}
        \subsection{Introduction}
        Point of statistics is to use sample information to infer data
        about the population. Populations are characterized by numbers
        (\textit{parameters}) and we often want to estimate the value
        of parameter(s). Parameters include the proportion $p$,
        population mean $\mu$, variance $\sigma^2$ and standard
        deviation $\sigma$.
        \begin{defn}
          The parameter of interest in an experiment is called the
          \textit{target parameter}.
        \end{defn}
        \begin{defn}
          A \textit{point estimate} is a type of estimate where we use
          a single value/point to estimate a parameter. If we estimate
          a parameter by saying that it might fall between two
          numbers, this is an \textit{interval estimate}. We can use
          information from the sample to calculate these estimates,
          which are done using an estimator.
        \end{defn}
        \begin{defn}
          An \textit{estimator} is a rule, often expressed as a
          formula, that tells how to calculate the value of an
          estimate based on the measurements contained in a sample.
        \end{defn}
        \begin{defn}
          \textit{Sample mean:}
          $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$$ This is an
          example point estimator of $\mu$.
        \end{defn}
        There can be different estimators for the same population
        parameter. Some estimators are considered good and others are bad.
        \subsection{The Bias and Mean Square Error of Point
          Estimators}
        We cannot measure how good a point estimation procedure is
        with a single estimate, we need to observe the procedure many
        times. We create a frequency distribution to measure the
        goodness of a point estimator.
        \paragraph{Point Estimators} For a population parameter
        $\theta$, the estimator of $\theta$ is called $\hat{\theta}$.
        \begin{defn}
        Ideally, we'd want $E(\hat{\theta}) = \theta$. Point
        estimators that satisfy this are called
        \textit{unbiased}. Otherwise, they are called \textit{biased},
        where the \textit{bias} is given by
        $B(\hat{\theta})=E(\hat{\theta})-\theta$
      \end{defn}
      In addition, we'd also like the estimator $V(\hat{\theta})$ to
      be as small as possible, since a smaller variance guarantees a
      higher fraction of estimators to be ``close'' to $\theta$. If
      two estimators are unbiased and everything else is equal other
      than variance, we prefer the one with smaller variance.
      \begin{defn}
        Another way to characterize goodness of a point estimator is
        via its \textit{mean square error},
        $$MSE(\hat{\theta}) = E[(\hat{\theta}-\theta)^2]$$
        Which is the average of the square of the distance between the
        estimator and its target parameter. It can be shown that:
        $$MSE(\hat{\theta})=V(\hat{\theta})+[B(\hat{\theta})]^2$$
      \end{defn}
      \subsection{Some Common Unbiased Point Estimators}
      \begin{defn}
        $\sigma_{\hat{\theta}}^2$ denotes the variance of the sampling
        distribution of the estimator $\hat{\theta}$. The standard
        deviation $\sigma_{\hat{\theta}} =
        \sqrt{\sigma_{\hat{\theta}}^2}$ is called the \textit{standard
        error} of the estimator $\hat{\theta}$.
    \end{defn}
    \paragraph{Common Point Estimators} If random samples are
    independent we have:
    \\
    \begin{tabular}{|c c c c c|}
      \hline Target Parameter $\theta$ & Sample Size(s) & Point
      Estimator $\hat{\theta}$ & $E(\hat{\theta})$ & Standard Error
      $\sigma_{\hat{\theta}}$
      \\ \hline $\mu$ & $n$ & $\overline{Y}$ & $\mu$ &
      $\frac{\sigma}{\sqrt{n}}$
      \\ $p$ & $n$ & $\hat{p} = \frac{Y}{n}$ & $p$ &
      $\sqrt{\frac{pq}{n}}$
      \\ $\mu_1 - \mu_2$ & $n_1, n_2$ & $\overline{Y}_1 -
      \overline{Y}_2$ & $\mu_1 - \mu_2$ &
      $\sqrt{\frac{\sigma_1^2}{n_1}+ \frac{\sigma_2^2}{n_2}}$
      \\ $p_1 - p_2$ & $n_1,n_2$ & $\hat{p}_1 - \hat{p}_2$ & $p_1 -
      p_2$ & $\sqrt{\frac{p_1q_1}{n_1}+\frac{p_2q_2}{n_2}}$
      \\ \hline
    \end{tabular}\\
    Note, these are all \textbf{unbiased} estimators. Note that the expected
    values and standard errors for $\overline{Y}$ and $\overline{Y}_1
    - \overline{Y}_2$ are valid for any distribution of the population
    and all four estimators are approximately normal for large $n$
    (see CLT).
    \begin{defn}
      The sample variances,
      $$S^2 = \dfrac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}$$
      $$S'^2 = \dfrac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}$$
      are unbiased ($S^2$) and biased ($S'^2$) estimators for $\sigma^2$.
    \end{defn}
    \subsection{Evaluating the Goodness of a Point Estimator}
    \begin{defn}
      The \textit{error of estimation} $\varepsilon$ is the distance
      between an estimator and its target parameter:
      $$\varepsilon = |\hat{\theta} - \theta|$$
    \end{defn}
    Note that $\hat{\theta}$ is a random variable, so the error of
    estimation is a random quantity and thus we can make probability
    statements about it (rather than saying exactly how big it is for
    some estimate).

    The probability $P(\varepsilon < b)$ represents the fraction of
    times, in repeated sampling that $\hat{\theta}$ falls within $b$
    units of $\theta$.

    Sometimes we want to find a value of $b$ such that $P(\varepsilon
    < b) =$ some value, say $0.90$.

    If $\hat{\theta}$ is unbiased we can find a bound on $\varepsilon$
    by expressing $b$ as a multiple of the standard error. Take for
    example, $b=k \sigma_{\hat{\theta}}$, where $k \geq 1$, then by
    Tchebysheff's we know that $P(\varepsilon < k
    \sigma_{\hat{\theta}}) \geq 1 -
    \frac{1}{k^2}$. $b=2\sigma_{\hat{\theta}}$ is a good approximate
    bound on $\varepsilon$ in practice. By Tchebysheff we know that
    $P(\varepsilon < 2\sigma_{\hat{\theta}}) \geq .75$.
    \subsection{Confidence Intervals}
    \begin{defn}
      An \textit{interval estimator} is a rule specifying the method
      for using sample measurements to calculate two numbers
      (endpoints) of the interval. We want the interval to contain
      $\theta$ and for it to be \textbf{narrow}. One or both of the
      endpoints are functions of the sample measurements and thus vary
      randomly from sample to sample, i.e. cannot be sure that
      $\theta$ falls in the interval, so we want an interval estimator
      with a high probability of containing $\theta$.
    \end{defn}
    \begin{defn}
      Interval estimators are commonly called \textit{confidence intervals}.
    \end{defn}
    \begin{defn}
      Upper/lower endpoints are called \textit{upper} and
      \textit{lower confidence limits}, sometimes denoted
      $\hat{\theta}_L$ and $\hat{\theta}_U$.
    \end{defn}
    \begin{defn}
      Probability that a random confidence interval will enclose
      $\theta$ is called the \textit{confidence coefficient}, denoted
      by $(1 - \alpha)$. In
      practice, this coefficient describes the fraction of time that
      random/repeated sampling intervals will contain $\theta$.

      We have: $$P(\hat{\theta}_L \leq \theta \leq \hat{\theta}_U) = 1 - \alpha$$
    \end{defn}
    \begin{defn}
      $[\hat{\theta}_L, \hat{\theta}_U]$ is called a \textit{two-sided
      confidence interval}. A \textit{one-sided confidence interval}
    satisfies $[\hat{\theta}_L,\infty)$, an upper one-sided confidence
    interval is $(-\infty, \hat{\theta}_U]$.
  \end{defn}
  \begin{defn}
    The \textit{pivotal method} is very useful for finding confidence
    intervals. We find a pivotal quantity that satisfies:
    \begin{enumerate}
    \item Being a function of sample measurements and $\theta$, where
      $\theta$ is the only unknown quantity.
    \item Probability distribution does not depend on $\theta$.
    \end{enumerate}
    We can then say:
    \begin{align*}
      P(a \leq Y \leq b) = p &\implies P(ca \leq cY \leq cb) = p
      \\ & \implies P(a+d \leq Y+d \leq b+d) = p
    \end{align*}
    i.e. A change of scale or translation of $Y$ does not change the
    probability. So knowing the probability distribution of a pivotal
    quantity, we can use these operations and get the interval.
  \end{defn}
  \subsection{Large-Sample Confidence Intervals}
  If $\theta = \mu, p, \mu_1 - \mu_2$ or $p_1 - p_2$ then for large
  samples $$Z =
  \frac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}\stackrel{approx}{\sim}Normal(0,1)$$
  Also, $Z$ is (approximately) a pivotal quantity.

  We also have:

  \begin{align*}
    100(1-\alpha)\% \text{ lower bound for }\theta = \hat{\theta}-z_{\alpha}\sigma_{\hat{\theta}}
    \\100(1-\alpha)\% \text{ upper bound for }\theta = \hat{\theta}+z_{\alpha}\sigma_{\hat{\theta}}
    \\ \implies 100(1-2\alpha)\%\theta = \hat{\theta}\pm z_{\alpha}\sigma_{\hat{\theta}}
  \end{align*}
  The important formula of this section is
  $$\hat{\theta} \pm z_{\frac{\alpha}{2}}\sigma_{\hat{\theta}}$$
  When $\theta = \mu, \hat{\theta} = \overline{Y}$ and
  $\sigma_{\hat{\theta}}^2 = \frac{\sigma^2}{n}$, then $\sigma^2$
  should be used if known, but if it is not known and $n$ is large, we
  can use $s^2$ instead. Similarly for $\sigma_1^2$ and
  $\sigma_2^2$, for $\theta = \mu_1 - \mu_2$. For $\theta = p$, we can
  replace $p$ by $\hat{p}$ for large $n$ (to be justified in Section 9.3).
  \subsection{Selecting the Sample Size}
  We want to obtain information at minimum cost. Note that the sample
  size $n$ controls the amount of relevant information in a
  sample. How many measurements should we include in our sample? We
  first need to know how much information we'd like to
  obtain. Specifically, how \textbf{accurate} should the estimate be?

  For example, if we'd like to estimate $\mu$ within $5$ units with
  probability $.95$: since approximately $95\%$ of the sample means
  will lie within $2\sigma_{\overline{Y}}$ of $\mu$, then we want
  $$2 \sigma_{\overline{Y}} = 5 \implies \frac{2\sigma}{\sqrt{n}} = 5
  \implies n = \frac{4\sigma^2}{25}$$
  \subsection{Small-Sample Confidence Intervals for $\mu$ and
    $\mu_1-\mu_2$}
  We assume in this section that our sample has a normal
  distribution. We want to approximate the population mean when we
  don't know the variance or the sample size is too small to apply
  large-sample techniques. But we can use:
\begin{flalign*}
  T & = \frac{\overline{Y}-\mu}{\frac{S}{\sqrt{n}}}
\end{flalign*}
which has a $t$ distribution with $(n-1)$ df. $T$ is a pivotal
quantity for $\mu$ and we can get:
$$P(-t_{\frac{\alpha}{2}} \leq T \leq t_{\frac{\alpha}{2}}) = 1 -
\alpha$$
The $t$ distribution is similar to the normal distribution, except the
tails are thicker.

\paragraph{Important Parameters and Confidence Intervals}
~\\
\begin{tabular}{{c c}}
  Parameter & Confidence Interval
  \\ \hline $\mu$ & $\overline{Y} \pm t_{\frac{\alpha}{2}} \left(\frac{S}{\sqrt{n}}\right), df = n - 1$
  \\ $\mu_1 - \mu_2$ & $(\overline{Y}_1 - \overline{Y}_2 \pm t_{\frac{\alpha}{2}}S_p \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}), df=n_1 + n_2 - 2$
\end{tabular}
Where $S_p^2 = \frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}$

Note that as $n$ gets large, so does $df$ and eventually, the $t$
distribution can be approximated by the standard normal distribution
(the small $n$ intervals become nearly equivalent to large $n$ samples
when $df > 30$).
\subsection{Confidence Intervals for $\sigma^2$}
Recall that $$\frac{\sum_{i=1}^n (Y_i -
  \overline{Y})^2}{\sigma^2}=\frac{(n-1)S^2}{\sigma^2} =
\frac{(n-1)S^2}{\sigma^2}$$ has a $\chi^2$ distribution with $n-1$
df.

Using the pivotal method, we get:
$$P \left[\chi_L^2 \leq \frac{(n-1)S^2}{\sigma^2} \leq \chi_U^2\right]
= 1 - \alpha $$
Since the $\chi^2$ density function is not symmetric, we get to chose
$\chi_L^2$ and $\chi_U^2$. We usually choose points that cut off equal
tail areas:
$$P \left[\chi_{1-(\frac{\alpha}{2})}^2 \leq \frac{(n-1)S^2}{\sigma^2} \leq \chi_{\left(\frac{\alpha}{2}\right)}^2\right]
= 1 - \alpha $$
Obtaining:
$$P \left[\frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2}}} \leq \sigma^2
  \leq
  \frac{(n-1)S^2}{\chi^2_{1-\left(\frac{\alpha}{2}\right)}}\right] = 1- \alpha$$
\paragraph{$100(1-\alpha)\%$ Confidence Interval for $\sigma^2$}
$$
\left(
  \frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2}}}, \frac{(n-1)S^2}{\chi^2_{1-
      \left(
        \frac{\alpha}{2}
      \right)}}
\right)$$
\subsection{Summary}
The main objective of most statistical investigations is to make
inferences about population parameters based on sample data. These
inferences are often estimated (point or interval estimates). Unbiased
estimators with small variances are preferred. Goodness of unbiased
$\hat{\theta}$ is measured via $\sigma_{\hat{\theta}}$ because error
of estimation is usually smaller than $2\sigma_{\hat{\theta}}$. The
MSE is small if the variance and bias is small.

Interval estimates of many parameters like $\mu$ and $p$ can be
derived from the normal distribution for large $n$ because of CLT. If
$n$ is small, we must assume normality of the population and use the
$t$ distribution for confidence intervals.

IF sample measurements are from a normal distribution, we can get a
confidence interval for $\sigma^2$ through the $\chi^2$ distribution. 
\end{document}